{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify weviate-client is installed and the database is live and ready\n",
    "import weaviate\n",
    "client = weaviate.Client(\"http://localhost:8080\")\n",
    "assert client.is_live()\n",
    "assert client.is_ready()\n",
    "client.get_meta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f672325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!! Delete data in Weaviate\n",
    "client.schema.delete_class(\"Wiki_Node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941363dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html2text\n",
    "text_maker = html2text.HTML2Text()\n",
    "text_maker.ignore_links = True\n",
    "text_maker.ignore_images = True\n",
    "\n",
    "def wiki_to_text(ancestors, title, labels, body):\n",
    "    body_text = text_maker.handle(body)\n",
    "    text =  f\"Title: {title}\\n\"\n",
    "    if ancestors: text += f\"Ancestors: {ancestors}\\n\" \n",
    "    if labels: text += f\"Labels: {ancestors}\\n\"\n",
    "    text += f\"{body_text}\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac2c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk cached Wiki pages\n",
    "import os\n",
    "import re\n",
    "n = 0\n",
    "for root, dirs, files in os.walk(\"../data/wiki\"):\n",
    "    for name in files:\n",
    "        filepath = os.path.join(root, name)\n",
    "        with open(filepath) as f:\n",
    "            link = f.readline().rstrip()\n",
    "            ancestors = f.readline().rstrip()\n",
    "            title = f.readline().rstrip()\n",
    "            labels = f.readline().rstrip()\n",
    "            body = re.sub('[\\n]+', '\\n', \"\".join(f.readlines()))\n",
    "            text = wiki_to_text(ancestors, title, labels, body)\n",
    "            if n<1:\n",
    "                print(name)\n",
    "                print(link)\n",
    "                print(text)\n",
    "            n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1562b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Documents from cached wiki files\n",
    "from llama_index.legacy import Document\n",
    "documents = []\n",
    "for root, dirs, files in os.walk(\"../data/wiki\"):\n",
    "    for name in files:\n",
    "        filepath = os.path.join(root, name)\n",
    "        with open(filepath) as f:\n",
    "            link = f.readline().rstrip()\n",
    "            ancestors = f.readline().rstrip()\n",
    "            title = f.readline().rstrip()\n",
    "            labels = f.readline().rstrip()\n",
    "            body = re.sub('[\\n]+', '\\n', \"\".join(f.readlines()))\n",
    "            text = wiki_to_text(ancestors, title, labels, body)\n",
    "            doc = Document(text, doc_id=name, extra_info={\"title\": title, \"link\": link})\n",
    "            documents.append(doc)\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae76fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4040a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.legacy import LLMPredictor, PromptHelper, ServiceContext\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from llama_index.legacy import LangchainEmbedding\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.2, model_name=\"gpt-3.5-turbo-0301\")\n",
    "llm_predictor = LLMPredictor(llm=llm)\n",
    "embed_model = LangchainEmbedding(OpenAIEmbeddings())\n",
    "\n",
    "max_input_size = 4096\n",
    "num_output = 256\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model, prompt_helper=prompt_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d776d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate embedding for all of the documents and save them into Weaviate\n",
    "from llama_index.legacy import GPTVectorStoreIndex\n",
    "from llama_index.vector_stores import WeaviateVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "class_prefix = \"Wiki\"\n",
    "vector_store = WeaviateVectorStore(weaviate_client=client, class_prefix=class_prefix)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# persists the vector_store into Weaviate\n",
    "index = GPTVectorStoreIndex.from_documents(documents, storage_context=storage_context, service_context=service_context)\n",
    "\n",
    "# persist the docstore and index_store\n",
    "# this is currently required although in theory Weaviate should be able to handle these as well\n",
    "storage_context.persist(persist_dir='../storage/index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18caa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_nodes(nodes):\n",
    "    docs_ids = set()\n",
    "    unique_nodes = list()\n",
    "    for node in nodes:\n",
    "        if node.node.ref_doc_id not in docs_ids:\n",
    "            docs_ids.add(node.node.ref_doc_id)\n",
    "            unique_nodes.append(node)\n",
    "    return unique_nodes\n",
    "        \n",
    "def print_response(response):\n",
    "    print(response.response)    \n",
    "    for node in get_unique_nodes(response.source_nodes):\n",
    "        print(f\"{node.node.extra_info['title']}\")\n",
    "        print(f\"\\t{node.node.extra_info['link']}\")\n",
    "        \n",
    "def query(question, n=5):   \n",
    "    query_engine = index.as_query_engine(similarity_top_k=n)\n",
    "    res = query_engine.query(question)\n",
    "    print_response(res)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.vector_stores.types import VectorStoreQueryMode\n",
    "from llama_index.legacy import ResponseSynthesizer\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index,\n",
    "    similarity_top_k=5,\n",
    "    vector_store_query_mode=VectorStoreQueryMode.HYBRID,\n",
    "    alpha=0.7,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "synth = ResponseSynthesizer.from_args()\n",
    "\n",
    "# construct query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=synth,\n",
    ")\n",
    "\n",
    "query(\"On what physical server is Nextflow Tower installed, and where will it be moving to?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddf0c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "query(\"What interest groups does Scientific Computing sponsor?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

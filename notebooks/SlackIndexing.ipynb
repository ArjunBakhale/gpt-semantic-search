{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fa2da29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "slack_token = os.environ.get('SLACK_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7279d1a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m slack_token \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mSLACK_TOKEN\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m channel_ids \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mhpc\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m documents \u001b[39m=\u001b[39m SlackReader(slack_token\u001b[39m=\u001b[39;49mslack_token)\u001b[39m.\u001b[39;49mload_data(channel_ids\u001b[39m=\u001b[39;49mchannel_ids)\n\u001b[1;32m      5\u001b[0m index \u001b[39m=\u001b[39m GPTListIndex\u001b[39m.\u001b[39mfrom_documents(documents)\n",
      "File \u001b[0;32m~/dev/gpt-semantic-search/env/lib/python3.9/site-packages/llama_index/readers/slack.py:188\u001b[0m, in \u001b[0;36mSlackReader.load_data\u001b[0;34m(self, channel_ids, reverse_chronological)\u001b[0m\n\u001b[1;32m    186\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m    187\u001b[0m \u001b[39mfor\u001b[39;00m channel_id \u001b[39min\u001b[39;00m channel_ids:\n\u001b[0;32m--> 188\u001b[0m     channel_content \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_channel(\n\u001b[1;32m    189\u001b[0m         channel_id, reverse_chronological\u001b[39m=\u001b[39;49mreverse_chronological\n\u001b[1;32m    190\u001b[0m     )\n\u001b[1;32m    191\u001b[0m     results\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    192\u001b[0m         Document(channel_content, extra_info\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mchannel\u001b[39m\u001b[39m\"\u001b[39m: channel_id})\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    194\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/dev/gpt-semantic-search/env/lib/python3.9/site-packages/llama_index/readers/slack.py:141\u001b[0m, in \u001b[0;36mSlackReader._read_channel\u001b[0;34m(self, channel_id, reverse_chronological)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mearliest_date_timestamp \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     conversations_history_kwargs[\u001b[39m\"\u001b[39m\u001b[39moldest\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[1;32m    139\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mearliest_date_timestamp\n\u001b[1;32m    140\u001b[0m     )\n\u001b[0;32m--> 141\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mconversations_history(\n\u001b[1;32m    142\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconversations_history_kwargs  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    143\u001b[0m )\n\u001b[1;32m    144\u001b[0m conversation_history \u001b[39m=\u001b[39m result[\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    145\u001b[0m \u001b[39m# Print results\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/gpt-semantic-search/env/lib/python3.9/site-packages/slack_sdk/web/client.py:2488\u001b[0m, in \u001b[0;36mWebClient.conversations_history\u001b[0;34m(self, channel, cursor, inclusive, include_all_metadata, latest, limit, oldest, **kwargs)\u001b[0m\n\u001b[1;32m   2474\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fetches a conversation's history of messages and events.\u001b[39;00m\n\u001b[1;32m   2475\u001b[0m \u001b[39mhttps://api.slack.com/methods/conversations.history\u001b[39;00m\n\u001b[1;32m   2476\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2477\u001b[0m kwargs\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m   2478\u001b[0m     {\n\u001b[1;32m   2479\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mchannel\u001b[39m\u001b[39m\"\u001b[39m: channel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2486\u001b[0m     }\n\u001b[1;32m   2487\u001b[0m )\n\u001b[0;32m-> 2488\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi_call(\u001b[39m\"\u001b[39;49m\u001b[39mconversations.history\u001b[39;49m\u001b[39m\"\u001b[39;49m, http_verb\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m, params\u001b[39m=\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/dev/gpt-semantic-search/env/lib/python3.9/site-packages/slack_sdk/web/base_client.py:156\u001b[0m, in \u001b[0;36mBaseClient.api_call\u001b[0;34m(self, api_method, http_verb, files, data, params, json, headers, auth)\u001b[0m\n\u001b[1;32m    141\u001b[0m req_args \u001b[39m=\u001b[39m _build_req_args(\n\u001b[1;32m    142\u001b[0m     token\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken,\n\u001b[1;32m    143\u001b[0m     http_verb\u001b[39m=\u001b[39mhttp_verb,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m     proxy\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproxy,\n\u001b[1;32m    153\u001b[0m )\n\u001b[1;32m    155\u001b[0m show_2020_01_deprecation(api_method)\n\u001b[0;32m--> 156\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sync_send(api_url\u001b[39m=\u001b[39;49mapi_url, req_args\u001b[39m=\u001b[39;49mreq_args)\n",
      "File \u001b[0;32m~/dev/gpt-semantic-search/env/lib/python3.9/site-packages/slack_sdk/web/base_client.py:187\u001b[0m, in \u001b[0;36mBaseClient._sync_send\u001b[0;34m(self, api_url, req_args)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    185\u001b[0m     body_params\u001b[39m.\u001b[39mupdate(data)\n\u001b[0;32m--> 187\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_urllib_api_call(\n\u001b[1;32m    188\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    189\u001b[0m     url\u001b[39m=\u001b[39;49mapi_url,\n\u001b[1;32m    190\u001b[0m     query_params\u001b[39m=\u001b[39;49m{},\n\u001b[1;32m    191\u001b[0m     body_params\u001b[39m=\u001b[39;49mbody_params,\n\u001b[1;32m    192\u001b[0m     files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    193\u001b[0m     json_body\u001b[39m=\u001b[39;49m_json,\n\u001b[1;32m    194\u001b[0m     additional_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    195\u001b[0m )\n",
      "File \u001b[0;32m~/dev/gpt-semantic-search/env/lib/python3.9/site-packages/slack_sdk/web/base_client.py:294\u001b[0m, in \u001b[0;36mBaseClient._urllib_api_call\u001b[0;34m(self, token, url, query_params, json_body, body_params, files, additional_headers)\u001b[0m\n\u001b[1;32m    291\u001b[0m     q \u001b[39m=\u001b[39m urlencode(query_params)\n\u001b[1;32m    292\u001b[0m     url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m&\u001b[39m\u001b[39m{\u001b[39;00mq\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m url \u001b[39melse\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m?\u001b[39m\u001b[39m{\u001b[39;00mq\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 294\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_perform_urllib_http_request(url\u001b[39m=\u001b[39;49murl, args\u001b[39m=\u001b[39;49mrequest_args)\n\u001b[1;32m    295\u001b[0m response_body \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)  \u001b[39m# skipcq: PTC-W0039\u001b[39;00m\n\u001b[1;32m    296\u001b[0m response_body_data: Optional[Union[\u001b[39mdict\u001b[39m, \u001b[39mbytes\u001b[39m]] \u001b[39m=\u001b[39m response_body\n",
      "File \u001b[0;32m~/dev/gpt-semantic-search/env/lib/python3.9/site-packages/slack_sdk/web/base_client.py:397\u001b[0m, in \u001b[0;36mBaseClient._perform_urllib_http_request\u001b[0;34m(self, url, args)\u001b[0m\n\u001b[1;32m    394\u001b[0m retry_state\u001b[39m.\u001b[39mnext_attempt_requested \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 397\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_perform_urllib_http_request_internal(url, req)\n\u001b[1;32m    398\u001b[0m     \u001b[39m# The resp is a 200 OK response\u001b[39;00m\n\u001b[1;32m    399\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry_handlers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/dev/gpt-semantic-search/env/lib/python3.9/site-packages/slack_sdk/web/base_client.py:526\u001b[0m, in \u001b[0;36mBaseClient._perform_urllib_http_request_internal\u001b[0;34m(self, url, req)\u001b[0m\n\u001b[1;32m    524\u001b[0m     resp \u001b[39m=\u001b[39m opener\u001b[39m.\u001b[39mopen(req, timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout)  \u001b[39m# skipcq: BAN-B310\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 526\u001b[0m     resp \u001b[39m=\u001b[39m urlopen(req, context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mssl, timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout)  \u001b[39m# skipcq: BAN-B310\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[39mif\u001b[39;00m resp\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget_content_type() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mapplication/gzip\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    528\u001b[0m     \u001b[39m# admin.analytics.getFile\u001b[39;00m\n\u001b[1;32m    529\u001b[0m     body: \u001b[39mbytes\u001b[39m \u001b[39m=\u001b[39m resp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/urllib/request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[0;32m--> 214\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/urllib/request.py:517\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    514\u001b[0m     req \u001b[39m=\u001b[39m meth(req)\n\u001b[1;32m    516\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[0;32m--> 517\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(req, data)\n\u001b[1;32m    519\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n\u001b[1;32m    520\u001b[0m meth_name \u001b[39m=\u001b[39m protocol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/urllib/request.py:534\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m    533\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[0;32m--> 534\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_open, protocol, protocol \u001b[39m+\u001b[39;49m\n\u001b[1;32m    535\u001b[0m                           \u001b[39m'\u001b[39;49m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m, req)\n\u001b[1;32m    536\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[1;32m    537\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/urllib/request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[1;32m    493\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 494\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    495\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/urllib/request.py:1389\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttps_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_open(http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mHTTPSConnection, req,\n\u001b[1;32m   1390\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context, check_hostname\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_hostname)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/urllib/request.py:1350\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[1;32m   1349\u001b[0m         \u001b[39mraise\u001b[39;00m URLError(err)\n\u001b[0;32m-> 1350\u001b[0m     r \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m   1351\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m   1352\u001b[0m     h\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1378\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1240\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1241\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1243\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from llama_index import GPTListIndex, SlackReader\n",
    "slack_token = os.getenv(\"SLACK_TOKEN\")\n",
    "channel_ids = [\"hpc\"]\n",
    "documents = SlackReader(slack_token=slack_token).load_data(channel_ids=channel_ids)\n",
    "index = GPTListIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ba4022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slack_sdk import WebClient\n",
    "client = WebClient(token=slack_token)\n",
    "res = client.api_test()\n",
    "if not res[\"ok\"]:\n",
    "    raise ValueError(f\"Error initializing Slack API: {res['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2981def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'W8FRV3WSD',\n",
       " 'team_id': 'T011UK0B5QD',\n",
       " 'name': 'parekhr',\n",
       " 'deleted': False,\n",
       " 'color': 'aba727',\n",
       " 'real_name': 'Ruchi Parekh',\n",
       " 'tz': 'America/New_York',\n",
       " 'tz_label': 'Eastern Daylight Time',\n",
       " 'tz_offset': -14400,\n",
       " 'profile': {'title': 'Director, Annotation & Analytics',\n",
       "  'phone': '+1 (571) 209-4000 x1444',\n",
       "  'skype': '',\n",
       "  'real_name': 'Ruchi Parekh',\n",
       "  'real_name_normalized': 'Ruchi Parekh',\n",
       "  'display_name': 'parekhr',\n",
       "  'display_name_normalized': 'parekhr',\n",
       "  'fields': None,\n",
       "  'status_text': '',\n",
       "  'status_emoji': '',\n",
       "  'status_emoji_display_info': [],\n",
       "  'status_expiration': 1604379599,\n",
       "  'avatar_hash': '4055dd671afd',\n",
       "  'image_original': 'https://avatars.slack-edge.com/2019-01-18/527300937698_4055dd671afd83c47254_original.jpg',\n",
       "  'is_custom_image': True,\n",
       "  'first_name': 'Ruchi',\n",
       "  'last_name': 'Parekh',\n",
       "  'image_24': 'https://avatars.slack-edge.com/2019-01-18/527300937698_4055dd671afd83c47254_24.jpg',\n",
       "  'image_32': 'https://avatars.slack-edge.com/2019-01-18/527300937698_4055dd671afd83c47254_32.jpg',\n",
       "  'image_48': 'https://avatars.slack-edge.com/2019-01-18/527300937698_4055dd671afd83c47254_48.jpg',\n",
       "  'image_72': 'https://avatars.slack-edge.com/2019-01-18/527300937698_4055dd671afd83c47254_72.jpg',\n",
       "  'image_192': 'https://avatars.slack-edge.com/2019-01-18/527300937698_4055dd671afd83c47254_192.jpg',\n",
       "  'image_512': 'https://avatars.slack-edge.com/2019-01-18/527300937698_4055dd671afd83c47254_512.jpg',\n",
       "  'image_1024': 'https://avatars.slack-edge.com/2019-01-18/527300937698_4055dd671afd83c47254_1024.jpg',\n",
       "  'status_text_canonical': '',\n",
       "  'team': 'T011UK0B5QD'},\n",
       " 'is_admin': False,\n",
       " 'is_owner': False,\n",
       " 'is_primary_owner': False,\n",
       " 'is_restricted': False,\n",
       " 'is_ultra_restricted': False,\n",
       " 'is_bot': False,\n",
       " 'is_app_user': False,\n",
       " 'updated': 1677854615,\n",
       " 'is_email_confirmed': True,\n",
       " 'has_2fa': False,\n",
       " 'who_can_share_contact_card': 'EVERYONE',\n",
       " 'enterprise_user': {'id': 'W8FRV3WSD',\n",
       "  'enterprise_id': 'E7T75T6FR',\n",
       "  'enterprise_name': 'HHMI',\n",
       "  'is_admin': False,\n",
       "  'is_owner': False,\n",
       "  'is_primary_owner': False,\n",
       "  'teams': ['T010K5MEU74',\n",
       "   'T011UK0B5QD',\n",
       "   'T017DDVPULA',\n",
       "   'T01MEEF3LE5',\n",
       "   'T01RQFBHCAK',\n",
       "   'T02063ME8MR',\n",
       "   'T028CKL99RV',\n",
       "   'T02LTA7JZC0',\n",
       "   'T03CU5Y4FPU',\n",
       "   'T0SP1EXMM',\n",
       "   'T0STLGC4R',\n",
       "   'T2FCMGXU2',\n",
       "   'T3EBQ3154',\n",
       "   'T3ZGM8JJU',\n",
       "   'T8RAZ5W3Z',\n",
       "   'TDBDJ2GHX',\n",
       "   'TK09SH99V']}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request = client.api_call(\"users.list\")\n",
    "request['members'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b3991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171 users\n"
     ]
    }
   ],
   "source": [
    "user_map = {}\n",
    "request = client.api_call(\"users.list\")\n",
    "if request['ok']:\n",
    "    for item in request['members']:\n",
    "        id = item['id']\n",
    "        username = item['name']\n",
    "        name = item['profile']['real_name']\n",
    "        user_map[id] = name\n",
    "\n",
    "print(f\"{len(user_map)} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d3a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Optional\n",
    "import time\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def _read_message(channel_id: str, message_ts: str) -> str:\n",
    "    from slack_sdk.errors import SlackApiError\n",
    "\n",
    "    \"\"\"Read a message.\"\"\"\n",
    "\n",
    "    messages_text: List[str] = []\n",
    "    next_cursor = None\n",
    "    while True:\n",
    "        try:\n",
    "            # https://slack.com/api/conversations.replies\n",
    "            # List all replies to a message, including the message itself.\n",
    "            result = client.conversations_replies(\n",
    "                channel=channel_id, ts=message_ts, cursor=next_cursor\n",
    "            )\n",
    "            messages = result[\"messages\"]\n",
    "            messages_text.extend(message[\"text\"] for message in messages)\n",
    "            if not result[\"has_more\"]:\n",
    "                break\n",
    "\n",
    "            next_cursor = result[\"response_metadata\"][\"next_cursor\"]\n",
    "        except SlackApiError as e:\n",
    "            if e.response[\"error\"] == \"ratelimited\":\n",
    "                logger.error(\n",
    "                    \"Rate limit error reached, sleeping for: {} seconds\".format(\n",
    "                        e.response.headers[\"retry-after\"]\n",
    "                    )\n",
    "                )\n",
    "                time.sleep(int(e.response.headers[\"retry-after\"]))\n",
    "            else:\n",
    "                logger.error(\"Error parsing conversation replies: {}\".format(e))\n",
    "\n",
    "    return \"\\n\\n\".join(messages_text)\n",
    "\n",
    "def _read_channel(channel_id: str, reverse_chronological: bool) -> str:\n",
    "    from slack_sdk.errors import SlackApiError\n",
    "\n",
    "    \"\"\"Read a channel.\"\"\"\n",
    "\n",
    "    result_messages: List[str] = []\n",
    "    next_cursor = None\n",
    "    while True:\n",
    "        try:\n",
    "            # Call the conversations.history method using the WebClient\n",
    "            # conversations.history returns the first 100 messages by default\n",
    "            # These results are paginated,\n",
    "            # see: https://api.slack.com/methods/conversations.history$pagination\n",
    "            conversations_history_kwargs = {\n",
    "                \"channel\": channel_id,\n",
    "                \"cursor\": next_cursor,\n",
    "            }\n",
    "            result = client.conversations_history(\n",
    "                **conversations_history_kwargs  # type: ignore\n",
    "            )\n",
    "            conversation_history = result[\"messages\"]\n",
    "            # Print results\n",
    "            logger.info(\n",
    "                \"{} messages found in {}\".format(\n",
    "                    len(conversation_history), channel_id\n",
    "                )\n",
    "            )\n",
    "            result_messages.extend(\n",
    "                _read_message(channel_id, message[\"ts\"])\n",
    "                for message in conversation_history\n",
    "            )\n",
    "            if not result[\"has_more\"]:\n",
    "                break\n",
    "            next_cursor = result[\"response_metadata\"][\"next_cursor\"]\n",
    "\n",
    "        except SlackApiError as e:\n",
    "            if e.response[\"error\"] == \"ratelimited\":\n",
    "                logger.error(\n",
    "                    \"Rate limit error reached, sleeping for: {} seconds\".format(\n",
    "                        e.response.headers[\"retry-after\"]\n",
    "                    )\n",
    "                )\n",
    "                time.sleep(int(e.response.headers[\"retry-after\"]))\n",
    "            else:\n",
    "                logger.error(\"Error parsing conversation replies: {}\".format(e))\n",
    "\n",
    "    return (\n",
    "        \"\\n\\n\".join(result_messages)\n",
    "        if reverse_chronological\n",
    "        else \"\\n\\n\".join(result_messages[::-1])\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e73bac29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Noticed by <@W94TA6021> and posted on <#C03CZHVDQF4|go> channel but probably even more pertinent to this channel since it hasn\\'t been mentioned AFAIK but is used by a number of companies for ML and data processing: <https://docs.flyte.org/en/latest/>\\n<https://github.com/flyteorg/flyte>\\n\\nDuring a volume EM data workgroup meeting (most participants based in Europe), a researcher working on a VDI solution for bioimage analysis talked about how the more \"computationally experienced\" people were using NextFlow for their workflows.   This BAND system is an experiment to test general computational access with virtual workstation display via VNC.\\n\\nI mention this because there was a question on how much NextFlow is being adopted for bioimaging in addition to its current bioinformatics concentration.\\n\\nWe\\'ve started discussing with EMBL and others on using Nextflow for bioimaging pipelines. There\\'s an ongoing chat here: <https://imagesc.zulipchat.com/#narrow/stream/367292-Nextflow>\\n\\nWe also have a <#C020ZKT8ZB9|nextflow> channel here.\\n\\n<@U04ES3WBN3V> has joined the channel\\n\\nThread on why \"bioinformatics workflow managers (Nextflow, Snakemake, CWL, WDL, Redun, etc) exist in parallel to data engineering workflow managers (Airflow, Dagster, Prefect).\"\\n<https://twitter.com/BenSiranosian/status/1558631901346164736?s=20&amp;t=faaWNB37i2LazFcnOvDFPA>\\n\\n*Nextflow Tower*\\nOfficial public server: <https://tower.nf>\\nJanelia internal server: <http://nextflow.int.janelia.org>\\n\\n*Recent review paper:*\\n_Approaches for containerized scientific workflows in cloud environments with applications in life science_:\\n<https://peerj.com/preprints/27141/>\\nTheir conclusions:\\n• Galaxy has a graphical user interface and might be more attractive for users with less expertise in programming/scripting.\\n• Nextflow is arguably the most versatile systems for working with containers being able to operator either on cloud and HPC batch schedulers.\\n• Pachyderm is build specifically for running on Kubernetes and has a data versioning file system built-in.\\n• Luigi/SciLuigi supports many data sources out of the box, including HDFS, S3, etc. and integrates seamlessly with Apache Hadoop and Spark.\\n• SciPipe implements an agile workflow programming API that smoothly integrates with the fast growing Go programming language ecosystem.\\n\\nA few workflow systems suggested in today\\'s DevOps chat:\\n<https://www.pachyderm.com>\\n<https://www.prefect.io>\\n\\n<@W94TA6021> has joined the channel\\n\\n<@W976239RN> has joined the channel\\n\\n<@W98102BPT> has joined the channel\\n\\n<@W9CLKL509> has joined the channel\\n\\n<@W014VCMKWLS> has joined the channel\\n\\n<@W98103T5X> has joined the channel\\n\\n<@W8WM9KJ15> has joined the channel\\n\\nset the channel description: Workflow systems, pipelines, and computational plugins\\n\\n<@W97NAKQN7> has joined the channel'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_id = \"C019ZPNLLA0\" # workflows\n",
    "channel_content = _read_channel(\n",
    "    channel_id, reverse_chronological=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07d7299f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noticed by <@W94TA6021> and posted on <#C03CZHVDQF4|go> channel but probably even more pertinent to this channel since it hasn't been mentioned AFAIK but is used by a number of companies for ML and data processing: <https://docs.flyte.org/en/latest/>\n",
      "<https://github.com/flyteorg/flyte>\n",
      "\n",
      "During a volume EM data workgroup meeting (most participants based in Europe), a researcher working on a VDI solution for bioimage analysis talked about how the more \"computationally experienced\" people were using NextFlow for their workflows.   This BAND system is an experiment to test general computational access with virtual workstation display via VNC.\n",
      "\n",
      "I mention this because there was a question on how much NextFlow is being adopted for bioimaging in addition to its current bioinformatics concentration.\n",
      "\n",
      "We've started discussing with EMBL and others on using Nextflow for bioimaging pipelines. There's an ongoing chat here: <https://imagesc.zulipchat.com/#narrow/stream/367292-Nextflow>\n",
      "\n",
      "We also have a <#C020ZKT8ZB9|nextflow> channel here.\n",
      "\n",
      "<@U04ES3WBN3V> has joined the channel\n",
      "\n",
      "Thread on why \"bioinformatics workflow managers (Nextflow, Snakemake, CWL, WDL, Redun, etc) exist in parallel to data engineering workflow managers (Airflow, Dagster, Prefect).\"\n",
      "<https://twitter.com/BenSiranosian/status/1558631901346164736?s=20&amp;t=faaWNB37i2LazFcnOvDFPA>\n",
      "\n",
      "*Nextflow Tower*\n",
      "Official public server: <https://tower.nf>\n",
      "Janelia internal server: <http://nextflow.int.janelia.org>\n",
      "\n",
      "*Recent review paper:*\n",
      "_Approaches for containerized scientific workflows in cloud environments with applications in life science_:\n",
      "<https://peerj.com/preprints/27141/>\n",
      "Their conclusions:\n",
      "• Galaxy has a graphical user interface and might be more attractive for users with less expertise in programming/scripting.\n",
      "• Nextflow is arguably the most versatile systems for working with containers being able to operator either on cloud and HPC batch schedulers.\n",
      "• Pachyderm is build specifically for running on Kubernetes and has a data versioning file system built-in.\n",
      "• Luigi/SciLuigi supports many data sources out of the box, including HDFS, S3, etc. and integrates seamlessly with Apache Hadoop and Spark.\n",
      "• SciPipe implements an agile workflow programming API that smoothly integrates with the fast growing Go programming language ecosystem.\n",
      "\n",
      "A few workflow systems suggested in today's DevOps chat:\n",
      "<https://www.pachyderm.com>\n",
      "<https://www.prefect.io>\n",
      "\n",
      "<@W94TA6021> has joined the channel\n",
      "\n",
      "<@W976239RN> has joined the channel\n",
      "\n",
      "<@W98102BPT> has joined the channel\n",
      "\n",
      "<@W9CLKL509> has joined the channel\n",
      "\n",
      "<@W014VCMKWLS> has joined the channel\n",
      "\n",
      "<@W98103T5X> has joined the channel\n",
      "\n",
      "<@W8WM9KJ15> has joined the channel\n",
      "\n",
      "set the channel description: Workflow systems, pipelines, and computational plugins\n",
      "\n",
      "<@W97NAKQN7> has joined the channel\n"
     ]
    }
   ],
   "source": [
    "print(channel_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e816cc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://janelia-dev.slack.com/archives/C057Z7J7F29/p1684259866667699\n"
     ]
    }
   ],
   "source": [
    "\n",
    "channel_id = \"C057Z7J7F29\"\n",
    "message_ts = \"1684259866.667699\"\n",
    "\n",
    "res = client.chat_getPermalink(channel=channel_id, message_ts=message_ts)\n",
    "if res['ok']:\n",
    "    print(res['permalink'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "808c72bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import glob\n",
    "import os \n",
    "\n",
    "data_path = \"../data/slack/slack_export_Janelia-Software_30days\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c788af75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170 users\n"
     ]
    }
   ],
   "source": [
    "id2username = {}\n",
    "id2realname = {}\n",
    "\n",
    "with open(f\"{data_path}/users.json\", 'r') as f:\n",
    "    users = json.load(f)\n",
    "    for user in users:\n",
    "        id = user['id']\n",
    "        id2username[id] = user['name']\n",
    "        id2realname[id] = user['profile']['real_name']\n",
    "\n",
    "print(f\"{len(id2username)} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee09b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C011TMUB3UP python\n",
      "C011W6YDV99 random\n",
      "C0128K68NE5 general\n",
      "C013E4ULBFU storage\n",
      "C013EB3CZM1 scientific-visualization\n",
      "C01430CRBHT git-github\n",
      "C0146BJ38PQ wednesday_web_workshop\n",
      "C015MJGSM2S julia\n",
      "C01H5PYR4TW hpc\n",
      "C01J3KE45LG rust\n",
      "C02CTFPCTDM cplusplus\n",
      "C02HDABKNAE code-review\n",
      "C02K818Q3B6 java\n",
      "C031U6KUMNU how-to\n",
      "C032XSC2CJC image_benchmarks\n",
      "C03DJGPC69K programming_languages\n",
      "C03S782CCMD architecture\n",
      "C041XB9U8BX applied-deep-learning\n",
      "C045UGQB4LX globus\n",
      "C049U3BDYPL mastodon\n",
      "C04UUTQVB61 wiki-improvement\n",
      "C057Z7J7F29 easi-fish-pipeline\n",
      "C02K252DJ86 chromatix\n"
     ]
    }
   ],
   "source": [
    "channel2id = {}\n",
    "with open(f\"{data_path}/channels.json\", 'r') as f:\n",
    "    channels = json.load(f)\n",
    "\n",
    "    for channel in channels:\n",
    "        print(f\"{channel['id']} {channel['name']}\")\n",
    "        channel2id[channel['name']] = channel['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa25f150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rokickik/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Well, this is a sentence.',\n",
       " 'And the U.S. is a country.',\n",
       " 'But is this a question?',\n",
       " 'What about if I mention the U.S.?']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "msg = \"Well, this is a sentence. And the U.S. is a country. But is this a question? What about if I mention the U.S.?\"\n",
    "nltk.tokenize.sent_tokenize(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d540bf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jody Clements said: Does Janelia sci comp have an organization on PyPI?\n",
      "It seems you came to the same basic conclusion at the moment, though?\n",
      "Did you think of a way?\n",
      "Davis Bennett said: but can you explain what you need concurrency for here?\n",
      "Davis Bennett said: but won't the user then be responsible for awaiting that future to get the results?\n",
      "Davis Bennett said: does python async allow nested event loops?\n",
      "Does my detection method work if I have h5py 3.8 installed and HDF5 1.12.2?\n",
      "Do we really need Zarr shards?\n",
      "What time?\n",
      "Mark Kittisopikul said: Why does this seem like an https://en.wikipedia.org/wiki/Embrace,_extend,_and_extinguish approach to me?\n",
      "Davis Bennett said: who is doing EEE here?\n",
      "Davis Bennett said: and what would get extinguished?\n",
      "Embrace, extend, and solidify?\n",
      "Why not develop Mojo in the open from the beginning?\n",
      "Davis Bennett said: looks like the guy who made `ruff` (very good linter) got some funding?\n",
      "Mark Kittisopikul said: What's the advantage of this being a magic extension?\n",
      "Mark Kittisopikul said: Why not just call `watermark()`?\n",
      "Mark Kittisopikul said: Can anyone figure out a way to do this without starting a thread?\n",
      "Davis Bennett said: so you basically want to load chunks in parallel?\n",
      "Davis Bennett said: what's wrong with the simple version then?\n",
      "Davis Bennett said: but if `chunk_iter` requires callbacks I don't see how you avoid callbacks here?\n",
      "Davis Bennett said: it doesn't look like `chunk_iter` returns anything?\n",
      "Davis Bennett said: but if you're stuck with it, I think the queue is fine?\n",
      "Davis Bennett said: does `chunk_iter` run on multiple threads?\n",
      "Davis Bennett said: but does the python API need to look like the C API?\n",
      "Davis Bennett said: at least in this case, the amount of data is small, so would it not be possible to use the C API with a callback that fills a dynamically-sized array, and then return that to python?\n",
      "Davis Bennett said: would this work?\n",
      "Davis Bennett said: in this case, what's so bad about loading it all at once?\n",
      "it's not a lot of data, no?\n",
      "Mark Kittisopikul said: In C?\n",
      "Mark Kittisopikul said: How many chunks do you usually deal with?\n",
      "Mark Kittisopikul said: Did `Py_UNICODE` leak into the Zarr v2 specification and is it still supported in Zarr v3?\n",
      "Mark Kittisopikul said: Does that mean implementations have to support UTF-16 and UTF-32 strings?\n",
      "Mark Kittisopikul said: Maybe a wasm sandbox might be appropriate to contain ChatGPT?\n",
      "Mary Lay said: Best way to set up mamba when creating environments on the cluster?\n",
      "I initially did `conda install mamba -n base -c conda-forge` , which isn't recommended, but how likely am I to run into issues going this route?\n",
      "And/or what's your solution to the long \"Solving Environment\" time for creating conda envs?\n",
      "Davis Bennett said: did the environment take a long time with mamba?\n",
      "Konrad Rokicki said: Maybe we could have a copy in /misc/sc for everyone to use?\n",
      "Am I the only one who thinks a corruption bug in a compression library might need to be addressed with some urgency?\n",
      "Mark Kittisopikul said: What is Anaconda selling at http://pyscript.com?\n",
      "Davis Bennett said: is this that \"python in the browser\" thing?\n",
      "Ben Arthur said: mark-- what can PySR do (if anything) than https://github.com/MilesCranmer/SymbolicRegression.jl can't?\n",
      "Adam Taylor said: So are pandas and numpy really declining in popularity?\n",
      "Mark Kittisopikul said: If it were an artifact, then why does it not affect the other lines?\n",
      "Why does it seem specific to pandas?\n",
      "Stuart Berg said: Who knows?\n",
      "Davis Bennett said: yeah, which is more likely?\n",
      "Adam Taylor said: What about numpy?\n",
      "Are people just shifting to things implemented on top of numpy, so fewer 'raw' numpy questions?\n",
      "Davis Bennett said:  What about numpy?\n",
      "Are people just shifting to things implemented on top of numpy, so fewer 'raw' numpy questions?\n",
      "Mark Kittisopikul said: Maybe there is a better source for pandas help?\n",
      "Mark Kittisopikul said: or numpy help?\n",
      "Do these plots count \"closed\" questions (such as questions tagged as \"duplicate\" or just poor)?\n",
      "Huh?\n",
      "?\n",
      "Davis Bennett said: for julia, is it possible that most of the conversation happens somewhere else?\n",
      "Davis Bennett said: isn't python strongly typed?\n",
      "Davis Bennett said: *or, if making an new language (like taichi I guess?\n",
      "Stuart Berg said: So can't Julia do more-or-less the same thing, then?\n",
      "Wrap Python and make it available in Julia?\n",
      "Or am I thinking of the opposite (wrap Julia from Python)?\n",
      "Mark Kittisopikul said: How so?\n",
      "Mark Kittisopikul said: What I really want to know is what is Modular's business model?\n",
      "Mark Kittisopikul said: We also think the relationship with CPython can build from both directions - wouldnâ€™t it be cool if the CPython team eventually reimplemented the interpreter in Mojo instead of C?Does anyone actually think that's going to happen?\n",
      "Mark Kittisopikul said: Has anyone seen a reaction from Guido van Rossum?\n",
      "Stuart Berg said: Does anyone actually think that's going to happen?\n",
      "If you are willing to break backwards compatibility, then that makes the problem a lot simpler -- but then what's the point?\n",
      "Davis Bennett said:  How so?\n",
      "William Katz said: What I really want to know is what is Modular's business model?\n",
      "Davis Bennett said: is all the `tex` stuff in `PATH`?\n",
      "Mark Kittisopikul said: Is it just me or is very difficult to install latex via conda?\n",
      "I know it's ugly and annoying, but...  Have you tried just stripping the minor version patch version from every requirement and hoping for the best?\n",
      "Stuart Berg said: and see what happens?\n",
      "Adam Taylor said: So did numpy peak in 2021?\n",
      "Mark Kittisopikul said: How do we add R to that plot?\n",
      "roughly-equally-popular OOP frameworks?\n",
      "Oh you thought that was a field selector, or maybe a message send?\n",
      "Mark Kittisopikul said: Who wants to guess how one gets the first element of a vector in R?\n",
      "Adam Taylor said: `car()`?\n",
      "Ben Arthur said: mamba took 15min?\n",
      "or conda??\n",
      "OMG, it uses `exec` on a generated code string??\n",
      "William Katz said: A VM that can't call outside its box?\n",
      "Cristian Goina said: what is the easiest way to do that?\n",
      "What's our operational definition of statically typed again and how is that applied here?\n",
      "Davis Bennett said: i think the normal definition applies?\n",
      "types are known before the code runs?\n",
      "Mark Kittisopikul said: When do we check that?\n",
      "Mark Kittisopikul said: So the static typing errors are reported at runtime?\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 90\u001b[0m\n\u001b[1;32m     88\u001b[0m documents \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m channel_name \u001b[38;5;129;01min\u001b[39;00m channel2id\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparse_channel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchannel_name\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     91\u001b[0m         documents\u001b[38;5;241m.\u001b[39mappend(doc)\n",
      "Cell \u001b[0;32mIn[15], line 82\u001b[0m, in \u001b[0;36mparse_channel\u001b[0;34m(channel_name)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(f):\n\u001b[1;32m     81\u001b[0m     msg \u001b[38;5;241m=\u001b[39m parse_message(message)\n\u001b[0;32m---> 82\u001b[0m     qs \u001b[38;5;241m=\u001b[39m \u001b[43mparse_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m qs:\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28mprint\u001b[39m(q)\n",
      "Cell \u001b[0;32mIn[15], line 69\u001b[0m, in \u001b[0;36mparse_questions\u001b[0;34m(msg)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_questions\u001b[39m(msg):\n\u001b[1;32m     68\u001b[0m     questions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m sentence[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     71\u001b[0m             questions\u001b[38;5;241m.\u001b[39mappend(sentence)\n",
      "File \u001b[0;32m~/dev/gpt-semantic-search/env/lib/python3.9/site-packages/nltk/tokenize/__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/gpt-semantic-search/env/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/dev/gpt-semantic-search/env/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[1;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m~/dev/gpt-semantic-search/env/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[1;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[0;32m~/dev/gpt-semantic-search/env/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[1;32m   1328\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[0;32m-> 1329\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[0;32m~/dev/gpt-semantic-search/env/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[1;32m   1460\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[0;32m~/dev/gpt-semantic-search/env/lib/python3.9/site-packages/nltk/tokenize/punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    319\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/gpt-semantic-search/env/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1431\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mslice\u001b[39m]:\n\u001b[1;32m   1430\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1431\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[1;32m   1432\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[1;32m   1433\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[0;32m~/dev/gpt-semantic-search/env/lib/python3.9/site-packages/nltk/tokenize/punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1393\u001b[0m previous_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1394\u001b[0m previous_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lang_vars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiod_context_re\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1396\u001b[0m \n\u001b[1;32m   1397\u001b[0m     \u001b[38;5;66;03m# Get the slice of the previous word\u001b[39;00m\n\u001b[1;32m   1398\u001b[0m     before_text \u001b[38;5;241m=\u001b[39m text[previous_slice\u001b[38;5;241m.\u001b[39mstop : match\u001b[38;5;241m.\u001b[39mstart()]\n\u001b[1;32m   1399\u001b[0m     index_after_last_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_last_whitespace_index(before_text)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "from decimal import *\n",
    "from nltk import tokenize\n",
    "\n",
    "ignored_subtypes = set(['channel_join','channel_leave'])\n",
    "\n",
    "def fix_text(text):\n",
    "    text = re.sub(\"&lt;\", \"<\", text)\n",
    "    text = re.sub(\"&gt;\", \">\", text)\n",
    "    text = re.sub(\"\\n+\", \"\\n\", text)\n",
    "    return text\n",
    "\n",
    "def get(element, key):\n",
    "    if element and key in element:\n",
    "        return element[key]\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_text(elements):\n",
    "    text = ''\n",
    "    for element in elements:\n",
    "        if 'elements' in element:\n",
    "            text += extract_text(element['elements'])\n",
    "        el_type = get(element, 'type')\n",
    "        if el_type == 'text':\n",
    "            if get(get(element, 'style'), 'code'): text += '`'\n",
    "            text += element['text']\n",
    "            if get(get(element, 'style'), 'code'): text += '`'\n",
    "        elif el_type == 'link':\n",
    "            text += get(element, 'url')\n",
    "        elif el_type == 'rich_text_preformatted':\n",
    "            text += \"\\n\"\n",
    "        elif el_type == 'user':\n",
    "            user_id = element['user_id']\n",
    "            try:\n",
    "                text += id2realname[user_id]\n",
    "            except KeyError:\n",
    "                #print(f\"ERROR: no such user {user_id}\")\n",
    "                text += user_id\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_message(message):\n",
    "    if get(message, 'type') == 'message':\n",
    "        if 'subtype' in message and get(message, 'subtype') in ignored_subtypes:\n",
    "            pass\n",
    "        else:\n",
    "            ts = message['ts']\n",
    "            thread_ts = get(message, 'thread_ts') or ts\n",
    "            msg_user = message['user']\n",
    "            try:\n",
    "                realname = id2realname[msg_user]\n",
    "            except KeyError:\n",
    "                realname = message['user_profile']['display_name']\n",
    "                \n",
    "            if 'blocks' in message:\n",
    "                text = extract_text(message['blocks'])\n",
    "            else:\n",
    "                text = message['text']\n",
    "            \n",
    "            text_msg = re.sub(\"<@(.*?)>\", lambda m: id2realname[m.group(1)], text)\n",
    "            text_msg = fix_text(text_msg)\n",
    "\n",
    "            text_msg = f\"{realname} said: {text_msg}\\n\"\n",
    "            return text_msg\n",
    "            \n",
    "def parse_questions(msg):\n",
    "    questions = []\n",
    "    for sentence in tokenize.sent_tokenize(msg):\n",
    "        if sentence[-1] == \"?\":\n",
    "            questions.append(sentence)\n",
    "    return questions\n",
    "    \n",
    "\n",
    "def parse_channel(channel_name):\n",
    "    channel_id = channel2id[channel_name]\n",
    "    messages = {}\n",
    "    for messages_file in glob.glob(f\"{data_path}/{channel_name}/*.json\"):\n",
    "        with open(messages_file, 'r') as f:\n",
    "            for message in json.load(f):\n",
    "                msg = parse_message(message)\n",
    "                qs = parse_questions(msg)\n",
    "                for q in qs:\n",
    "                    print(q)\n",
    "\n",
    "    return documents\n",
    "\n",
    "documents = []\n",
    "for channel_name in channel2id.keys():\n",
    "    for doc in parse_channel(channel_name):\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d884d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b6c6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

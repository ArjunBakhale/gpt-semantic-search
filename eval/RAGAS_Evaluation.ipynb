{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2690737a",
   "metadata": {},
   "source": [
    "Purpose: Turn text-data into data with appropriate values neccessary for functional RAGAS evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe365bb",
   "metadata": {},
   "source": [
    "Fetch documents (from only the SciComp wiki as of now) and put into accessable format for generation of expected outputs and creation of context later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ef27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"WEB LOADER\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import bs4 as bs\n",
    "import html2text\n",
    "from llama_index.core import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "data_path = '../data/janelia.org'  # Use './' to indicate the current directory\n",
    "text_maker = html2text.HTML2Text()\n",
    "text_maker.ignore_links = True\n",
    "text_maker.images_to_alt = True\n",
    "text_maker.single_line_break = True\n",
    "text_maker.ignore_emphasis = True\n",
    "SOURCE = \"Web\"\n",
    "\n",
    "\n",
    "def webpage_to_text(soup):\n",
    "    \"\"\" Convert a generic web page to searchable text\n",
    "    \"\"\"\n",
    "    title = soup.title.text\n",
    "    text = text_maker.handle(str(soup))\n",
    "    return title,text\n",
    "\n",
    "\n",
    "def janelia_org_to_text(soup):\n",
    "    \"\"\" Convert a janelia.org page to searchable text\n",
    "    \"\"\"\n",
    "    title = soup.title.text.replace(\" | Janelia Research Campus\",\"\")\n",
    "    content_sections = soup.find_all(\"section\", class_=\"content-section\")\n",
    "    if not content_sections:\n",
    "        return title,None\n",
    "    if len(content_sections) > 1:\n",
    "        raise Exception(\"More than one content section\")\n",
    "    content = content_sections[0]\n",
    "    # Remove useless content\n",
    "    for div in content.find_all(\"div\", {'class':['panels-ipe-label','secondary_menu']}):\n",
    "        div.decompose()\n",
    "    # Html2text smashes text together if only tags separate it\n",
    "    # This fix not only adds the spacing but also adds a separator for nav buttons\n",
    "    for span in content.find_all(\"span\", {'class':'button-wrapper'}):\n",
    "        sep = bs.NavigableString(\" / \")\n",
    "        span.insert(0, sep)\n",
    "    text = text_maker.handle(str(content))\n",
    "    return title,text\n",
    "\n",
    "\n",
    "def html_to_text(link, body):\n",
    "    \"\"\" Convert a web page to plain text for use as a GPT prompt.\n",
    "    \"\"\"\n",
    "    soup = bs.BeautifulSoup(body,'lxml')\n",
    "    if \"janelia.org\" in link:\n",
    "        title,text = janelia_org_to_text(soup)\n",
    "    else:\n",
    "        title,text = webpage_to_text(soup)\n",
    "    return title,text\n",
    "\n",
    "\n",
    "class WebSiteLoader():\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def create_document(self, name, title, link, doc_text):\n",
    "        metadata = {\"source\": self.data_path, \"title\": title, \"link\": link}\n",
    "        # Debugging: Print doc_text to ensure it's not empty\n",
    "        return [Document(page_content=doc_text, metadata=metadata)]\n",
    "    \n",
    "    def load_all_documents(self):\n",
    "        documents = []\n",
    "        for root, dirs, files in os.walk(self.data_path):\n",
    "            for name in files:\n",
    "                filepath = os.path.join(root, name)\n",
    "                with open(filepath) as f:\n",
    "                    link = f.readline().strip()\n",
    "                    body = f.read()\n",
    "                    title, text = html_to_text(link, body)\n",
    "                    \n",
    "                    \n",
    "                    # print(f\"Title: {title}\")\n",
    "                    # print(f\"Text: {text}\")\n",
    "                    if text:\n",
    "                        final_text = title + \"\\n\" + text\n",
    "                        with open('tempTestGen.txt', 'w') as file:\n",
    "                            file.write(final_text)\n",
    "                        loader = TextLoader(\"./tempTestGen.txt\")\n",
    "                        doc = loader.load()\n",
    "                        documents.append(doc)\n",
    "        return documents\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Open output.txt in write mode\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31d58422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ARCHIVED WIKI LOADRER\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "import html2text\n",
    "from llama_index.core import Document\n",
    "\n",
    "warnings.simplefilter(\"ignore\", ResourceWarning)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "SOURCE = \"Wiki\"\n",
    "\n",
    "text_maker = html2text.HTML2Text()\n",
    "text_maker.ignore_links = True\n",
    "text_maker.ignore_images = True\n",
    "\n",
    "\n",
    "def wiki_to_text(ancestors, title, authors, labels, body):\n",
    "    \"\"\" Convert a wiki document to plain text for use as a GPT prompt.\n",
    "    \"\"\"\n",
    "    body_text = text_maker.handle(body)\n",
    "    text =  f\"Title: {title}\\n\"\n",
    "    if authors: text += f\"Authors: {authors}\\n\" \n",
    "    if ancestors: text += f\"Ancestors: {ancestors}\\n\" \n",
    "    if labels: text += f\"Labels: {ancestors}\\n\"\n",
    "    text += f\"{body_text}\"\n",
    "    return text\n",
    "\n",
    "\n",
    "class WikiLoader():\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def create_document(self, name, title, link, doc_text):\n",
    "        metadata = {\"source\": self.data_path, \"title\": title, \"link\": link}\n",
    "        return [Document(page_content=doc_text, metadata=metadata)]\n",
    "\n",
    "    def load_all_documents(self):\n",
    "        documents = []\n",
    "        for root, dirs, files in os.walk(self.data_path):\n",
    "            for name in files:\n",
    "                filepath = os.path.join(root, name)\n",
    "                with open(filepath) as f:\n",
    "                    link = f.readline().rstrip()\n",
    "                    ancestors = f.readline().rstrip()\n",
    "                    title = f.readline().rstrip()\n",
    "                    authors = f.readline().rstrip()\n",
    "                    labels = f.readline().rstrip()\n",
    "                    body = re.sub('[\\n]+', '\\n', \"\".join(f.readlines()))\n",
    "                    text = wiki_to_text(ancestors, title, authors, labels, body)\n",
    "                    # doc = self.create_document(name, title, link, text)\n",
    "                    # documents.append(doc)\n",
    "                    if text:\n",
    "                        final_text = title + \"\\n\" + text\n",
    "                        with open('tempTestGen.txt', 'w') as file:\n",
    "                            file.write(final_text)\n",
    "                        loader = TextLoader(\"./tempTestGen.txt\")\n",
    "                        doc = loader.load()\n",
    "                        documents.append(doc)\n",
    "        return documents\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d48ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ARCHIVED SLACK LOADER\"\"\"\n",
    "import argparse\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "from decimal import Decimal\n",
    "\n",
    "from llama_index.core import Document\n",
    "\n",
    "warnings.simplefilter(\"ignore\", ResourceWarning)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "SOURCE = \"Slack\"\n",
    "DOCUMENT_PAUSE_SECS = 300\n",
    "IGNORED_SUBTYPES = set(['channel_join','channel_leave','bot_message'])\n",
    "\n",
    "\n",
    "def get(dictionary, key):\n",
    "    \"\"\" Get the key out of the dictionary, if it exists. If not, return None.\n",
    "    \"\"\"\n",
    "    if dictionary and key in dictionary:\n",
    "        return dictionary[key]\n",
    "    return None\n",
    "\n",
    "\n",
    "def fix_text(text):\n",
    "    \"\"\" Standard transformations on text like squashing multiple newlines.\n",
    "    \"\"\"\n",
    "    text = re.sub(\"\\n+\", \"\\n\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "class ArchivedSlackLoader():\n",
    "\n",
    "    def __init__(self, data_path, debug=False):\n",
    "        self.data_path = data_path\n",
    "        self.id2username = {}\n",
    "        self.id2realname = {}\n",
    "        self.channel2id = {}\n",
    "        self.debug = debug\n",
    "\n",
    "        for user in self.get_users():\n",
    "            id = user['id']\n",
    "            self.id2username[id] = user['name']\n",
    "            self.id2realname[id] = user['profile']['real_name']\n",
    "\n",
    "        logger.info(f\"Loaded {len(self.id2username)} users\")\n",
    "        for channel in self.get_channels():\n",
    "            logger.debug(f\"{channel['id']}: {channel['name']}\")\n",
    "            self.channel2id[channel['name']] = channel['id']\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.channel2id)} channels\")\n",
    "\n",
    "\n",
    "    def get_users(self):\n",
    "        \"\"\" Generator which returns users from the users.json file.\n",
    "        \"\"\"\n",
    "        with open(f\"{self.data_path}/users.json\", 'r') as f:\n",
    "            users = json.load(f)\n",
    "            for user in users:\n",
    "                yield user\n",
    "\n",
    "\n",
    "    def get_channels(self):\n",
    "        \"\"\" Generator which returns channels from the channels.json file.\n",
    "        \"\"\"\n",
    "        with open(f\"{self.data_path}/channels.json\", 'r') as f:\n",
    "            channels = json.load(f)\n",
    "            for channel in channels:\n",
    "                yield channel\n",
    "\n",
    "\n",
    "    def get_messages(self, channel_name):\n",
    "        \"\"\" Generator which returns messages from the json files in the given channel directory.\n",
    "        \"\"\"\n",
    "        for messages_file in glob.glob(f\"{self.data_path}/{channel_name}/*.json\"):\n",
    "            with open(messages_file, 'r') as f:\n",
    "                for message in json.load(f):\n",
    "                    yield message\n",
    "\n",
    "\n",
    "    def extract_text(self, elements):\n",
    "        \"\"\" Recursively parse an 'elements' structure, \n",
    "            converting user elements to their real names.\n",
    "        \"\"\"\n",
    "        text = ''\n",
    "        for element in elements:\n",
    "            if 'elements' in element:\n",
    "                text += self.extract_text(element['elements'])\n",
    "            el_type = get(element, 'type')\n",
    "            if el_type == 'text':\n",
    "                if get(get(element, 'style'), 'code'): text += '`'\n",
    "                text += element['text']\n",
    "                if get(get(element, 'style'), 'code'): text += '`'\n",
    "            elif el_type == 'link':\n",
    "                text += get(element, 'url')\n",
    "            elif el_type == 'rich_text_preformatted':\n",
    "                text += \"\\n\"\n",
    "            elif el_type == 'user':\n",
    "                user_id = element['user_id']\n",
    "                try:\n",
    "                    text += self.id2realname[user_id]\n",
    "                except KeyError:\n",
    "                    logger.error(f\"No such user '{user_id}'\")\n",
    "                    text += user_id\n",
    "\n",
    "        return text\n",
    "\n",
    "    def parse_message(self, message):\n",
    "        \"\"\" Parse a message into text that will be read by a GPT model. \n",
    "        \"\"\"\n",
    "        thread_id, text_msg = None, None\n",
    "        if get(message, 'type') == 'message':\n",
    "            if 'subtype' in message and get(message, 'subtype') in IGNORED_SUBTYPES:\n",
    "                pass\n",
    "            else:\n",
    "                ts = message['ts']\n",
    "                thread_ts = get(message, 'thread_ts') or ts\n",
    "                thread_id = Decimal(thread_ts)\n",
    "\n",
    "                # Translate user\n",
    "                user_id = message['user']\n",
    "                try:\n",
    "                    realname = self.id2realname[user_id]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        realname = message['user_profile']['display_name']\n",
    "                    except KeyError:\n",
    "                        realname = user_id\n",
    "                    \n",
    "                if 'blocks' in message:\n",
    "                    text = self.extract_text(message['blocks'])\n",
    "                else:\n",
    "                    text = message['text']\n",
    "                \n",
    "                text_msg = re.sub(\"<@(.*?)>\", lambda m: self.id2realname[m.group(1)], text)\n",
    "                text_msg = fix_text(text_msg)\n",
    "\n",
    "                if 'attachments' in message:\n",
    "                    for attachment in message['attachments']:\n",
    "                        if 'title' in attachment: text_msg += f\"\\n{fix_text(attachment['title'])}\"\n",
    "                        if 'text' in attachment: text_msg += f\"\\n{fix_text(attachment['text'])}\"\n",
    "                        \n",
    "                if 'files' in message:\n",
    "                    for file in message['files']:\n",
    "                        if 'name' in file:\n",
    "                            # There are several cases where a file doesn't have a name:\n",
    "                            # 1) The file has been deleted (mode=tombstone)\n",
    "                            # 2) We have no access (file_access=access_denied)\n",
    "                            text_msg += f\"\\n<{file['name']}>\"\n",
    "\n",
    "                if 'reactions' in message:\n",
    "                    text_msg += f\"\\nOthers reacted to the previous message with \"\n",
    "                    r = [f\"{reaction['name']} a total of {reaction['count']} times\" for reaction in message['reactions']]\n",
    "                    text_msg += \", and with \".join(r) + \".\"\n",
    "\n",
    "                text_msg = f\"{realname} said: {text_msg}\\n\"\n",
    "        \n",
    "        return thread_id, text_msg\n",
    "\n",
    "\n",
    "    def create_document(self, channel_id, ts, doc_text):\n",
    "        logger.info(\"--------------------------------------------------\")\n",
    "        logger.info(f\"Document[channel={channel_id},ts={ts}]\")\n",
    "        logger.debug(doc_text)\n",
    "        return Document(text=doc_text, extra_info={\"source\": SOURCE, \"channel\": channel_id, \"ts\": ts})\n",
    "\n",
    "\n",
    "    def load_documents(self, channel_name):\n",
    "        channel_id = self.channel2id[channel_name]\n",
    "        messages = {}\n",
    "        for message in self.get_messages(channel_name):\n",
    "            try:\n",
    "                thread_id, text_msg = self.parse_message(message)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error parsing message: {message}\")\n",
    "                raise e\n",
    "                \n",
    "            if thread_id and text_msg:\n",
    "                if thread_id not in messages:\n",
    "                    messages[thread_id] = []\n",
    "                messages[thread_id].append(text_msg)\n",
    "\n",
    "        prev_id = Decimal(0)\n",
    "        documents = []\n",
    "        doc_text = \"\"\n",
    "        start_ts = None\n",
    "\n",
    "        for thread_id in sorted(list(messages.keys())):\n",
    "\n",
    "            # Create a new document whenever messages are separated by a longer pause\n",
    "            if doc_text and thread_id-prev_id > DOCUMENT_PAUSE_SECS:\n",
    "                doc = self.create_document(channel_id, start_ts, doc_text)\n",
    "                documents.append(doc)\n",
    "                doc_text = \"\"\n",
    "                start_ts = None\n",
    "\n",
    "            logger.debug(thread_id)\n",
    "\n",
    "            # Starting timestamp for the next document\n",
    "            if not start_ts:\n",
    "                start_ts = str(thread_id)\n",
    "\n",
    "            # Add all messages from the current thread\n",
    "            for text_msg in messages[thread_id]:\n",
    "                doc_text += text_msg\n",
    "\n",
    "            prev_id = thread_id\n",
    "\n",
    "        # Add final document\n",
    "        doc = self.create_document(channel_id, start_ts, doc_text)\n",
    "        documents.append(doc)\n",
    "\n",
    "        return documents\n",
    "\n",
    "\n",
    "    def load_all_documents(self):\n",
    "        documents = []\n",
    "        for channel_name in self.channel2id.keys():\n",
    "            for doc in self.load_documents(channel_name):\n",
    "                documents.append(doc)\n",
    "        return documents\n",
    "\n",
    "data_path = '../data/slack/janelia-software/slack_to_2023-05-18'\n",
    "loader = ArchivedSlackLoader(data_path)\n",
    "documents = loader.load_all_documents()\n",
    "with open('documents.txt', 'w') as file:\n",
    "    for document in documents:\n",
    "        file.write(str(document) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e912570b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.3.1 available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"DOCUMENT LOADER ALL SOURCES\"\"\"\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "# generator with openai models\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.llms import Ollama\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import os\n",
    "\n",
    "# DONE: Recursively load all scraped files in the directory and its subdirectories\n",
    "# loader = TextLoader(\"./test.txt\")\n",
    "# documents = loader.load()\n",
    "# print(documents)\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import mimetypes\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class DocumentLoader:\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def load_documents(self):\n",
    "        all_documents = []\n",
    "        for folder_name in os.listdir(self.root_dir):\n",
    "            folder_path = os.path.join(self.root_dir, folder_name)\n",
    "            if os.path.isdir(folder_path):\n",
    "                if folder_name == \"wiki\":\n",
    "                    loader = WikiLoader(folder_path)\n",
    "                # elif folder_name == \"slack\":\n",
    "                #     loader = ArchivedSlackLoader(folder_path)\n",
    "                elif folder_name == \"janelia.com\":\n",
    "                    loader = WebSiteLoader(folder_path)\n",
    "                else:\n",
    "                    continue  # Skip if folder doesn't match any criteria\n",
    "                documents = loader.load_all_documents()\n",
    "                all_documents.extend(documents)\n",
    "        return all_documents\n",
    "\n",
    "# Assuming your data folder is at \"./data/\"\n",
    "loader = DocumentLoader(\"../data\")\n",
    "documents = loader.load_documents()\n",
    "print (documents)\n",
    "# Assuming your data folder is at \"../data/\"\n",
    "with open('documents.txt', 'w') as file:\n",
    "    for document in documents:\n",
    "        file.write(str(document) + '\\n')\n",
    "# Assuming documents is a list of strings or convertible to string\n",
    "\n",
    "\n",
    "# Now `final_df` contains all the generated testsets in one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_llm = Ollama(model=\"llama3\")\n",
    "critic_llm = Ollama(model=\"llama3\")\n",
    "\n",
    "# generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "# critic_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ") \n",
    "\n",
    "\n",
    "\n",
    "# Initialize an empty list to collect all testsets\n",
    "all_testsets = []\n",
    "\n",
    "# Iterate over each document\n",
    "for document in documents:\n",
    "    # Generate a testset for the current document\n",
    "    current_testset = generator.generate_with_langchain_docs(document, test_size=10, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})\n",
    "    \n",
    "    # Assuming `current_testset` is a list of dictionaries where each dictionary represents a row\n",
    "    # Convert the current testset to a DataFrame and append it to the list\n",
    "    all_testsets.append(pd.DataFrame(current_testset))\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "final_df = pd.concat(all_testsets, ignore_index=True)\n",
    "df = pd.read_parquet('testset.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1a0410",
   "metadata": {},
   "source": [
    "Generate the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import logging\n",
    "# import sys\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# logging.getLogger('llama_index').setLevel(logging.DEBUG)\n",
    "# logging.getLogger('openai').setLevel(logging.DEBUG)\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "#change the model to a better one once the whole thing is functional\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56adcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = testset.to_pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5717751c",
   "metadata": {},
   "source": [
    "Address potentiall duplicate questions (which the testset generator has done) and delete their rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d241aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.drop_duplicates(subset='question', keep='first')\n",
    "questions_list = df['question'].tolist()\n",
    "print (questions_list)\n",
    "\n",
    "seen = set()\n",
    "questions_list = [x for x in questions_list if not (x in seen or seen.add(x))]\n",
    "\n",
    "for item in questions_list:\n",
    "    print(item)\n",
    "\n",
    "    \n",
    "\n",
    "# testset.to_json(\"testset.json\")\n",
    "# Creates dataset of ground truths contexts and questions for the testset\n",
    "# Missing answers column \n",
    "# On one medium size document, it took about 1 minutes to generate 10 questions and cost 3 dollars on OpenAI\n",
    "# Seems expensive when using gpt-4, is its use justified or does 3.5 get the job done?\n",
    "# Evaluate gpt-4 vs gpt-3.5-turbo-16k for RAGAS evaluation test data generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0a0523",
   "metadata": {},
   "source": [
    "fetch the LLM's response and append to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03839e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming SemanticSearchParser is your class and it has a method named 'generate_response' that takes a question and returns an answer\n",
    "\n",
    "# Initialize your SemanticSearchParser class\n",
    "# Adjust this step if your class initialization requires different parameters\n",
    "\n",
    "\n",
    "# Now you can import the class\n",
    "\n",
    "from generate_answer import SemanticSearchService\n",
    "\n",
    "weaviate_url = \"http://localhost:8777\"\n",
    "service = SemanticSearchService(weaviate_url)\n",
    "print (questions_list)\n",
    "\n",
    "# List to store answers (optional)\n",
    "answers_list = []\n",
    "\n",
    "\n",
    "# Loop through each question in the questions_list\n",
    "for question in questions_list:\n",
    "    # Use the question as input to get the answer\n",
    "    answer = service.generate_response(question)\n",
    "    \n",
    "    # Print the answer\n",
    "    \n",
    "    # Optionally, append the answer to answers_list for further processing\n",
    "    answers_list.append(answer)\n",
    "\n",
    "print (answers_list)\n",
    "# temp_ans_list = [\"The Janelia Scientific Computing team operates and maintains a world-class computational infrastructure that includes a high-performance compute cluster with over 5000 cores and 300 GPUs. This infrastructure is used to analyze and mine the large amounts of data produced by Janelia's scientists. The team also supports a state-of-the-art storage and compute infrastructure across two data centers, which currently supports over 15 petabytes of scientific data. This data is split across various storage tiers and is connected with an optical fiber ring. The team also maintains a 4500 sq ft data center with significant power and cooling capacity. \\n\\nIn addition to hardware, the team also has deep software skills in a broad range of programming languages, extendable applications, frameworks, cloud & cluster technologies, and databases. These skills are used to help with research and engineering tasks, from quick questions to full software life cycle support. The team's software skills, combined with their domain knowledge in areas such as image processing, machine learning, data handling, microscopy, instrument control, 3D graphics & visualization, and bioinformatics & transcriptomics, allow them to efficiently work with both experimentalists and computer scientists. \\n\\nThe team also develops and maintains a variety of tools and projects, such as NeuronBridge, HortaCloud, VVD Viewer, EASI-FISH pipeline, Render, RS-FISH, and BigStitcher, which are used for various aspects of data analysis and simulation in biological research.\", \"The Janelia Scientific Computing team provides a wide range of support for advanced imaging techniques and image analysis. They offer consultation on experiment design as well as image visualization and processing. They also provide comprehensive image and data analysis support for multiple software packages through hands-on assistance and/or custom-written macros/plugins/scripts for ImageJ/FIJI, MATLAB, Imaris, etc. \\n\\nIn addition, they maintain several computer workstations dedicated to viewing and processing large image datasets acquired with the facility's instruments. These workstations are equipped with a suite of imaging software, including a full version of Imaris, and have robust hardware specifications to handle large datasets. \\n\\nThe team also has deep domain knowledge in image processing, machine learning, data handling, and 3D graphics & visualization, which allows them to efficiently work with experimentalists and computer scientists in various research areas.\", \"The Janelia Scientific Computing team provides world-class computational infrastructure to support the institute's scientific endeavors. They operate and maintain all of Janelia’s storage and associated backup infrastructure, high performance compute cluster, and all Linux systems. They also manage Janelia’s data center and backup and disaster recovery resources. The team supports a Linux compute cluster with over 5000 cores and 300 GPUs, and is responsible for maintaining many other Linux servers and workstations. They also handle a significant amount of data, with almost 100TB of new Janelia’s data being safely backed up every month.\\n\\nIn addition to infrastructure, the Scientific Computing Software team works closely with Janelia's labs and project teams, providing everything from answering quick questions to full software life cycle support. They have a broad range of software skills, including programming languages, extendable applications, frameworks, cloud & cluster technologies, and databases. They also have deep domain knowledge in areas like image processing, machine learning, data handling, microscopy, instrument control, 3D graphics & visualization, and bioinformatics & transcriptomics. \\n\\nThe team also identifies opportunities for code reuse, reducing development overhead and support costs across Janelia. They are strong proponents of open science and have created the Open Science Software Initiative. Most of their software is open source and available via GitHub. They also run a Scientific Computing Associates program to embed associates in SciComp and the lab or team they work with. \\n\\nThe team is led by Stephan Preibisch and consists of three teams: Software Engineering headed by Konrad Rokicki, Computational Methods and Solutions, both headed by Stephan Preibisch. They have developed several tools and projects like NeuronBridge, HortaCloud, VVD Viewer, EASI-FISH pipeline, Render, RS-FISH, and BigStitcher. \\n\\nIn summary, the Janelia Scientific Computing team supports the institute's mission and drives innovation in modern biological research by providing robust computational infrastructure, software support, and developing innovative tools and solutions.\", 'The Janelia Scientific Computing team supports advanced imaging techniques in neuroscience and cell biology through a variety of ways. They have deep domain knowledge in image processing, machine learning, data handling, electron and light microscopy, instrument control, 3D graphics & visualization, bioinformatics & transcriptomics. They also develop and maintain a range of software tools and applications that aid in these areas. Some of these tools include NeuronBridge for finding neuron matches across modalities, HortaCloud for cloud-based collaborative annotation, VVD Viewer for volumetric rendering of 3D/4D microscopy data, and BigStitcher for efficient alignment of multi-tile and multi-angle image datasets. They also work closely with labs and project teams, providing full software life cycle support.', \"The Janelia Scientific Computing team supports modern biological research in several ways. They maintain a world-class computational infrastructure, including storage and backup infrastructure, a high-performance compute cluster, and all Linux systems. They also manage Janelia's data center and backup and disaster recovery resources. The team supports data storage infrastructure for storing and accessing scientific data, with over 15 petabytes of scientific data split across various storage tiers. They also support a Linux compute cluster with over 5000 cores and 300 GPUs, and maintain many other Linux servers and workstations. \\n\\nIn addition to infrastructure support, the Scientific Computing Software team works closely with Janelia's labs, project teams, and shared resources to help with research and engineering tasks. They provide everything from answering quick questions to full software life cycle support. The team's software skills span a broad range of programming languages, extendable applications, frameworks, cloud & cluster technologies, and databases. They also have deep domain knowledge in image processing, machine learning, data handling, electron and light microscopy, instrument control, 3D graphics & visualization, bioinformatics & transcriptomics. \\n\\nThe team also develops and maintains a variety of tools and projects, such as NeuronBridge, HortaCloud, VVD Viewer, EASI-FISH pipeline, Render, RS-FISH, and BigStitcher. They are strong proponents of open science and most of their software is open source and available via GitHub. They also run the Scientific Computing Associates program, which offers challenging assignments for those interested in computational science.\", \"The Janelia Scientific Computing team provides comprehensive support for advanced imaging techniques in neuroscience, cell biology, and bioinformatics through a variety of collaborations and custom software tools. They work closely with Janelia's labs, project teams, and shared resources to assist with research and engineering tasks. This can range from answering quick questions to providing full software life cycle support.\\n\\nThe team's software skills cover a broad range of programming languages, extendable applications, frameworks, cloud & cluster technologies, and databases. They have deep domain knowledge in image processing, machine learning, data handling, electron and light microscopy, instrument control, 3D graphics & visualization, bioinformatics & transcriptomics. Many team members have backgrounds in biology, enabling them to work efficiently with experimentalists and computer scientists.\\n\\nThe team is also involved in various projects and tools such as NeuronBridge, HortaCloud, VVD Viewer, EASI-FISH pipeline, Render, RS-FISH, and BigStitcher, which are designed to support advanced imaging techniques and data analysis in neuroscience, cell biology, and bioinformatics.\\n\\nFurthermore, the team is a strong proponent of open science and has teamed up with the Computation & Theory research area to create the Open Science Software Initiative. Most of their software is open source and available via GitHub, promoting collaboration and knowledge sharing. They also run the Scientific Computing Associates program, which embeds associates in SciComp and the lab or team they work with.\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9699a438",
   "metadata": {},
   "source": [
    "Add values from the list of JaneliaGPT responses to the dataframe[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eaea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['answer'] = None\n",
    "# Assuming df is your DataFrame and answers_list is a list with values to populate the 'Answer' column\n",
    "if len(df) == len(answers_list):\n",
    "    df['answer'] = answers_list\n",
    "else:\n",
    "    print(\"The length of answers_list does not match the number of rows in the DataFrame.\")\n",
    "\n",
    "df.dropna(axis=1, how='all', inplace=True)\n",
    "display(df)\n",
    "# df.to_json('WithAnswersDatasetFromTestTxt.json', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095ca61",
   "metadata": {},
   "source": [
    "Preprocess the dataframe to conver to a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55166bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempt to fix the issue with the answers_list not bsjdkflasdjjfklasdkl fasdjkfh jkas\n",
    "df_fix = df\n",
    "# List of columns to keep\n",
    "columns_to_keep = ['question', 'ground_truth', 'answer', 'contexts']\n",
    "\n",
    "# Reassign df to a DataFrame containing only the columns to keep\n",
    "\n",
    "df_fix= df_fix[columns_to_keep]\n",
    "\n",
    "\n",
    "\n",
    "# Assuming df is already defined and contains the necessary columns\n",
    "\n",
    "# Convert 'question' and 'answer' to lists of strings if they are not already\n",
    "# df_fix['question'] = df_fix['question'].apply(lambda x: [x] if isinstance(x, str) else x)\n",
    "# df_fix['answer'] = df_fix['answer'].apply(lambda x: [x] if isinstance(x, str) else x)\n",
    "\n",
    "# Ensure 'contexts' and 'ground_truth' are lists of lists of strings\n",
    "# This step assumes 'contexts' and 'ground_truth' are already in the correct format\n",
    "# If not, you would need to apply a similar conversion as above, ensuring each element is a list\n",
    "\n",
    "# Example conversion if 'contexts' and 'ground_truth' were not already lists of lists\n",
    "df_fix['contexts'] = df_fix['contexts'].apply(lambda x: [[y] for y in x] if isinstance(x, list) and all(isinstance(y, str) for y in x) else x)\n",
    "df_fix['ground_truth'] = df_fix['ground_truth'].apply(lambda x: [[y] for y in x] if isinstance(x, list) and all(isinstance(y, str) for y in x) else x)\n",
    "display(df_fix)\n",
    "# Now df should be in the correct format for training\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "amnesty_qa = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v2\")\n",
    "\n",
    "from datasets import Dataset\n",
    "dataset_fix = Dataset.from_pandas(df_fix)\n",
    "dataset_fix = dataset_fix.remove_columns(['__index_level_0__'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e5b0c",
   "metadata": {},
   "source": [
    "Convert dataframe to Dataset and compare to a vaild Dataset for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec82fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Features\n",
    "\n",
    "# Assuming dataset_fix and amnesty_qa[\"eval\"] are your datasets\n",
    "features_dataset_fix = dataset_fix.features\n",
    "# features_amnesty_eval = amnesty_qa[\"eval\"].features\n",
    "def format_columns(example):\n",
    "    # Format 'question', 'answer', and 'ground_truths' columns to single values\n",
    "    for column in ['ground_truth', 'answer', 'question']:\n",
    "        if column in example and example[column]:\n",
    "            example[column] = example[column]\n",
    "    \n",
    "    # Correctly format 'contexts' column to a list of list of strings\n",
    "    if 'contexts' in example:\n",
    "        # Ensure 'contexts' is a list of strings (not a list of lists)\n",
    "        if isinstance(example['contexts'], list):\n",
    "            # If the items are lists (or other non-string types), flatten and convert to strings\n",
    "            example['contexts'] = [str(item) for sublist in example['contexts'] for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "        else:\n",
    "            # If 'contexts' is not a list, convert it into a list of a single string\n",
    "            example['contexts'] = [str(example['contexts'])]\n",
    "\n",
    "    \n",
    "    return example\n",
    "\n",
    "\n",
    "# Apply the transformation to both datasets\n",
    "dataset_fix = dataset_fix.map(format_columns)\n",
    "# Direct comparison of data types\n",
    "\"\"\"if set(features_dataset_fix.keys()) == set(features_amnesty_eval.keys()):\n",
    "    all_types_match = True\n",
    "    for key in features_dataset_fix.keys():\n",
    "        type_dataset_fix = type(features_dataset_fix[key]).__name__\n",
    "        type_amnesty_eval = type(features_amnesty_eval[key]).__name__\n",
    "        if type_dataset_fix != type_amnesty_eval:\n",
    "            print(f\"Data type for feature '{key}' differs between datasets. dataset_fix: {type_dataset_fix}, amnesty_qa['eval']: {type_amnesty_eval}\")\n",
    "            all_types_match = False\n",
    "    if all_types_match:\n",
    "        print(\"The data types of all features in both datasets match.\")\n",
    "else:\n",
    "    print(\"The Features of the datasets differ in their keys.\")\"\"\"\n",
    "\"\"\"\n",
    "print (dataset_fix[\"question\"])\n",
    "print (dataset_fix[\"answer\"])\n",
    "print (dataset_fix[\"ground_truth\"])\n",
    "print (dataset_fix[\"contexts\"])\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6505f4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric\n",
    ")\n",
    "\n",
    "contextual_precision = ContextualPrecisionMetric()\n",
    "contextual_recall = ContextualRecallMetric()\n",
    "contextual_relevancy = ContextualRelevancyMetric()\n",
    "\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval import evaluate\n",
    "modified_items = []\n",
    "\n",
    "# Step 2: Iterate over each item in dataset_fix\n",
    "for item in dataset_fix:\n",
    "    # Step 3: Create a new LLMTestCase instance with modified fields\n",
    "    modified_item = LLMTestCase(\n",
    "        input=item[\"question\"],\n",
    "        actual_output=item[\"answer\"],\n",
    "        expected_output=item[\"ground_truth\"],\n",
    "        retrieval_context=item[\"contexts\"]\n",
    "    )\n",
    "    # Step 4: Append the modified item to the list\n",
    "    modified_items.append(modified_item)\n",
    "# Assuming dataset_fix supports item assignment\n",
    "\n",
    "\n",
    "\n",
    "evaluate(\n",
    "    test_cases=[modified_items],\n",
    "    metrics=[contextual_precision, contextual_recall, contextual_relevancy]\n",
    ")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2cc907",
   "metadata": {},
   "source": [
    "Run evaluation to gather the below metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be48f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "\n",
    "from ragas import evaluate\n",
    "\n",
    "eval_llm = Ollama(model=\"llama3\")\n",
    "# Will reutrn a dataframe with the metrics\n",
    "# Returns error for now because answers column is missing\n",
    "# Error is misleading, fix dataset first and make it match the docs example dataset\n",
    "\n",
    "result = evaluate(\n",
    "    dataset_fix,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "    ],\n",
    "    llm=eval_llm,\n",
    ")\n",
    "\n",
    "result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22595923",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "result.to_pandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36276909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import json\n",
    "import os\n",
    "\n",
    "class MetricsManager:\n",
    "    def __init__(self, file_path='metrics.json'):\n",
    "        self.file_path = file_path\n",
    "        self.metrics = self.load_metrics()\n",
    "\n",
    "    def load_metrics(self):\n",
    "        if os.path.exists(self.file_path):\n",
    "            with open(self.file_path, 'r') as file:\n",
    "                return json.load(file)\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "    def save_metrics(self):\n",
    "        with open(self.file_path, 'w') as file:\n",
    "            json.dump(self.metrics, file, indent=4)\n",
    "\n",
    "    def update_metrics(self, trial_name, context_precision, faithfulness, answer_relevancy, context_recall):\n",
    "        averages = {\n",
    "            'context_precision': context_precision,\n",
    "            'faithfulness': faithfulness,\n",
    "            'answer_relevancy': answer_relevancy,\n",
    "            'context_recall': context_recall\n",
    "        }\n",
    "        self.metrics[trial_name] = averages\n",
    "        self.save_metrics()\n",
    "\n",
    "    def render_table(self):\n",
    "        if not self.metrics:\n",
    "            print(\"No metrics available for plotting.\")\n",
    "            return\n",
    "\n",
    "        trials, metrics, averages = [], [], []\n",
    "        for trial_name, metrics_averages in self.metrics.items():\n",
    "            for metric, average in metrics_averages.items():\n",
    "                trials.append(trial_name)\n",
    "                metrics.append(metric)\n",
    "                averages.append(round(average, 4))\n",
    "\n",
    "        Eval_Categories = pd.DataFrame({\n",
    "            'Trial': trials,\n",
    "            'Metric': metrics,\n",
    "            'Average': averages\n",
    "        })\n",
    "\n",
    "        if Eval_Categories.empty:\n",
    "            print(\"No metrics data to plot.\")\n",
    "            return\n",
    "\n",
    "        fig = px.bar(\n",
    "            Eval_Categories,\n",
    "            x='Trial',\n",
    "            y='Average',\n",
    "            color='Metric',\n",
    "            barmode='group',\n",
    "            text='Average',\n",
    "            category_orders={\"Metric\": [\"context_precision\", \"faithfulness\", \"answer_relevancy\", \"context_recall\"]},\n",
    "            labels={\n",
    "                \"Average\": \"Average Score\",\n",
    "                \"Metric\": \"Metric\",\n",
    "                \"Trial\": \"Trial Name\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            width=1000,\n",
    "            height=600,\n",
    "            title=\"<b>Average Scores of Evaluation Metrics by Trial</b>\",\n",
    "            xaxis_title=\"Trial Name\",\n",
    "            yaxis_title=\"Average Score\",\n",
    "            font=dict(size=15)\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "MetricsManager()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

import pandas as pd
import asyncio
import asyncio
import logging
import ragas
from ragas import evaluate
from ragas.metrics import (
    answer_relevancy,
    faithfulness,
    context_recall,
    context_precision,
)
from datasets import load_dataset
from datasets import Dataset
from langchain_community.llms import Ollama
from datasets import Features
from datasets import Features
from ragas.metrics import (
    answer_relevancy,
    faithfulness,
    context_recall,
    context_precision,
)

from langchain_community.embeddings import OllamaEmbeddings
from ragas import exceptions
from ragas import evaluate
import asyncio
import logging
import ragas
import threading






def format_columns(example):
    # Format 'question', 'answer', and 'ground_truths' columns to single values
    for column in ['ground_truth', 'answer', 'question']:
        if column in example and example[column]:
            example[column] = example[column]
    
    # Correctly format 'contexts' column to a list of list of strings
    if 'contexts' in example:
        # Ensure 'contexts' is a list of strings (not a list of lists)
        if isinstance(example['contexts'], list):
            # If the items are lists (or other non-string types), flatten and convert to strings
            example['contexts'] = [str(item) for sublist in example['contexts'] for item in (sublist if isinstance(sublist, list) else [sublist])]
        else:
            # If 'contexts' is not a list, convert it into a list of a single string
            example['contexts'] = [str(example['contexts'])]

    
    return example

async def run_evaluation(dataset_fix, eval_llm, eval_embeddings):

    try:
        result = await evaluate(
            dataset_fix,
            metrics=[
                context_precision,
                faithfulness, 
                answer_relevancy,
                context_recall,
            ],
            llm=eval_llm,
            embeddings=eval_embeddings,
            raise_exceptions=False  # Add this to prevent exceptions from stopping execution
        )
        df_result = result.to_pandas()
        df_result.to_parquet('results.parquet')
        return df_result
    except ragas.exceptions.ExceptionInRunner as e:
        logging.info(f"Evaluation failed: {e}")
    except asyncio.TimeoutError:
        logging.info("Evaluation timed out")
    except Exception as e:
        logging.info(f"An unexpected error occurred: {e}")
    return None

def thread_entry(dataset_fix, eval_llm, eval_embeddings):
    asyncio.set_event_loop(asyncio.new_event_loop())  # Set a new event loop for the thread
    loop = asyncio.get_event_loop()
    try:
        result = loop.run_until_complete(run_evaluation(dataset_fix, eval_llm, eval_embeddings))
        if result is not None:
            print("Evaluation completed successfully")
        else:
            print("Evaluation failed or returned no results")
    finally:
        loop.close()

def main():
    df_fix = pd.read_parquet('withAnswersTestset.parquet')

    dataset_fix = Dataset.from_pandas(df_fix)
    dataset_fix = dataset_fix.map(format_columns)
    eval_llm = Ollama(model="llama3")
    eval_embeddings = OllamaEmbeddings(
        model="avr/sfr-embedding-mistral", 
    )   
    eval_thread = threading.Thread(target=thread_entry, args=(dataset_fix, eval_llm, eval_embeddings))
    eval_thread.start()
    eval_thread.join()

if __name__ == "__main__":
    main()

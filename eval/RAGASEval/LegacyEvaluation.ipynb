{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2690737a",
   "metadata": {},
   "source": [
    "Purpose: Turn text-data into data with appropriate values neccessary for functional RAGAS evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe365bb",
   "metadata": {},
   "source": [
    "Fetch documents (from only the SciComp wiki as of now) and put into accessable format for generation of expected outputs and creation of context later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e912570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import TextLoader\n",
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# DONE: Recursively load all scraped files in the directory and its subdirectories\n",
    "loader = TextLoader(\"../data/test/test.txt\")\n",
    "documents = loader.load()\n",
    "print(documents)\n",
    "\n",
    "# from langchain_community.document_loaders import TextLoader\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import glob\n",
    "\n",
    "# import glob\n",
    "# import os\n",
    "# import json\n",
    "# import mimetypes\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# class RecursiveTextLoader:\n",
    "#     def __init__(self, root_dir):\n",
    "#         self.root_dir = root_dir\n",
    "\n",
    "#     def load(self):\n",
    "#         documents = []\n",
    "#         for filepath in glob.glob(os.path.join(self.root_dir, '**'), recursive=True):\n",
    "#             if os.path.isfile(filepath):\n",
    "#                 mime_type, _ = mimetypes.guess_type(filepath)\n",
    "#                 if mime_type == 'application/json':\n",
    "#                     with open(filepath, 'r', encoding='utf-8') as file:\n",
    "#                         documents.append(json.load(file))\n",
    "#                 elif mime_type == 'text/html' or self._looks_like_html(filepath):\n",
    "#                     with open(filepath, 'r', encoding='utf-8') as file:\n",
    "#                         soup = BeautifulSoup(file, 'html.parser')\n",
    "#                         documents.append(soup.get_text())\n",
    "#                 elif mime_type == 'text/plain':\n",
    "#                     with open(filepath, 'r', encoding='utf-8') as file:\n",
    "#                         documents.append(file.read())\n",
    "#         return documents\n",
    "\n",
    "#     def _looks_like_html(self, filepath):\n",
    "#         with open(filepath, 'r', encoding='utf-8') as file:\n",
    "#             head = file.read(1024)  # Read the first 1024 bytes\n",
    "#             return '<html' in head or '<!DOCTYPE html>' in head\n",
    "\n",
    "# # Assuming your data folder is at \"../data/\"\n",
    "# loader = RecursiveTextLoader(\"../data/\")\n",
    "# documents = loader.load()\n",
    "# # Assuming documents is a list of strings or convertible to string\n",
    "# with open('output.txt', 'a', encoding='utf-8') as file:\n",
    "#     for document in documents:\n",
    "#         # Remove all linebreaks from the document and then append\n",
    "#         file.write(str(document).replace('\\n', '') + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1a0410",
   "metadata": {},
   "source": [
    "Generate the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "import os\n",
    "# generator with openai models\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load .env file\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "#change the model to a better one once the whole thing is functional\n",
    "\n",
    "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", api_key=api_key)\n",
    "# critic_llm = ChatOpenAI(model=\"gpt-4\")\n",
    "critic_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", api_key=api_key)\n",
    "embeddings = OpenAIEmbeddings(api_key=api_key)\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ") \n",
    "\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(documents, test_size=10, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5717751c",
   "metadata": {},
   "source": [
    "Address potentiall duplicate questions (which the testset generator has done) and delete their rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d241aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = testset.to_pandas()\n",
    "df = df.drop_duplicates(subset='question', keep='first')\n",
    "questions_list = df['question'].tolist()\n",
    "print (questions_list)\n",
    "\n",
    "seen = set()\n",
    "questions_list = [x for x in questions_list if not (x in seen or seen.add(x))]\n",
    "\n",
    "for item in questions_list:\n",
    "    print(item)\n",
    "\n",
    "    \n",
    "\n",
    "# testset.to_json(\"testset.json\")\n",
    "# Creates dataset of ground truths contexts and questions for the testset\n",
    "# Missing answers column \n",
    "# On one medium size document, it took about 1 minutes to generate 10 questions and cost 3 dollars on OpenAI\n",
    "# Seems expensive when using gpt-4, is its use justified or does 3.5 get the job done?\n",
    "# Evaluate gpt-4 vs gpt-3.5-turbo-16k for RAGAS evaluation test data generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb23bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('testset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0a0523",
   "metadata": {},
   "source": [
    "fetch the LLM's response and append to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03839e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming SemanticSearchParser is your class and it has a method named 'generate_response' that takes a question and returns an answer\n",
    "\n",
    "# Initialize your SemanticSearchParser class\n",
    "# Adjust this step if your class initialization requires different parameters\n",
    "\n",
    "\n",
    "# Now you can import the class\n",
    "\n",
    "from generate_answer import SemanticSearchService\n",
    "\n",
    "weaviate_url = \"http://localhost:8777\"\n",
    "service = SemanticSearchService(weaviate_url)\n",
    "print (questions_list)\n",
    "\n",
    "# List to store answers (optional)\n",
    "answers_list = []\n",
    "\n",
    "\n",
    "# Loop through each question in the questions_list\n",
    "for question in questions_list:\n",
    "    # Use the question as input to get the answer\n",
    "    answer = service.generate_response(question)\n",
    "    \n",
    "    # Print the answer\n",
    "    \n",
    "    # Optionally, append the answer to answers_list for further processing\n",
    "    answers_list.append(answer)\n",
    "\n",
    "print (answers_list)\n",
    "temp_ans_list = [\"The Janelia Scientific Computing team operates and maintains a world-class computational infrastructure that includes a high-performance compute cluster with over 5000 cores and 300 GPUs. This infrastructure is used to analyze and mine the large amounts of data produced by Janelia's scientists. The team also supports a state-of-the-art storage and compute infrastructure across two data centers, which currently supports over 15 petabytes of scientific data. This data is split across various storage tiers and is connected with an optical fiber ring. The team also maintains a 4500 sq ft data center with significant power and cooling capacity. \\n\\nIn addition to hardware, the team also has deep software skills in a broad range of programming languages, extendable applications, frameworks, cloud & cluster technologies, and databases. These skills are used to help with research and engineering tasks, from quick questions to full software life cycle support. The team's software skills, combined with their domain knowledge in areas such as image processing, machine learning, data handling, microscopy, instrument control, 3D graphics & visualization, and bioinformatics & transcriptomics, allow them to efficiently work with both experimentalists and computer scientists. \\n\\nThe team also develops and maintains a variety of tools and projects, such as NeuronBridge, HortaCloud, VVD Viewer, EASI-FISH pipeline, Render, RS-FISH, and BigStitcher, which are used for various aspects of data analysis and simulation in biological research.\", \"The Janelia Scientific Computing team provides a wide range of support for advanced imaging techniques and image analysis. They offer consultation on experiment design as well as image visualization and processing. They also provide comprehensive image and data analysis support for multiple software packages through hands-on assistance and/or custom-written macros/plugins/scripts for ImageJ/FIJI, MATLAB, Imaris, etc. \\n\\nIn addition, they maintain several computer workstations dedicated to viewing and processing large image datasets acquired with the facility's instruments. These workstations are equipped with a suite of imaging software, including a full version of Imaris, and have robust hardware specifications to handle large datasets. \\n\\nThe team also has deep domain knowledge in image processing, machine learning, data handling, and 3D graphics & visualization, which allows them to efficiently work with experimentalists and computer scientists in various research areas.\", \"The Janelia Scientific Computing team provides world-class computational infrastructure to support the institute's scientific endeavors. They operate and maintain all of Janelia’s storage and associated backup infrastructure, high performance compute cluster, and all Linux systems. They also manage Janelia’s data center and backup and disaster recovery resources. The team supports a Linux compute cluster with over 5000 cores and 300 GPUs, and is responsible for maintaining many other Linux servers and workstations. They also handle a significant amount of data, with almost 100TB of new Janelia’s data being safely backed up every month.\\n\\nIn addition to infrastructure, the Scientific Computing Software team works closely with Janelia's labs and project teams, providing everything from answering quick questions to full software life cycle support. They have a broad range of software skills, including programming languages, extendable applications, frameworks, cloud & cluster technologies, and databases. They also have deep domain knowledge in areas like image processing, machine learning, data handling, microscopy, instrument control, 3D graphics & visualization, and bioinformatics & transcriptomics. \\n\\nThe team also identifies opportunities for code reuse, reducing development overhead and support costs across Janelia. They are strong proponents of open science and have created the Open Science Software Initiative. Most of their software is open source and available via GitHub. They also run a Scientific Computing Associates program to embed associates in SciComp and the lab or team they work with. \\n\\nThe team is led by Stephan Preibisch and consists of three teams: Software Engineering headed by Konrad Rokicki, Computational Methods and Solutions, both headed by Stephan Preibisch. They have developed several tools and projects like NeuronBridge, HortaCloud, VVD Viewer, EASI-FISH pipeline, Render, RS-FISH, and BigStitcher. \\n\\nIn summary, the Janelia Scientific Computing team supports the institute's mission and drives innovation in modern biological research by providing robust computational infrastructure, software support, and developing innovative tools and solutions.\", 'The Janelia Scientific Computing team supports advanced imaging techniques in neuroscience and cell biology through a variety of ways. They have deep domain knowledge in image processing, machine learning, data handling, electron and light microscopy, instrument control, 3D graphics & visualization, bioinformatics & transcriptomics. They also develop and maintain a range of software tools and applications that aid in these areas. Some of these tools include NeuronBridge for finding neuron matches across modalities, HortaCloud for cloud-based collaborative annotation, VVD Viewer for volumetric rendering of 3D/4D microscopy data, and BigStitcher for efficient alignment of multi-tile and multi-angle image datasets. They also work closely with labs and project teams, providing full software life cycle support.', \"The Janelia Scientific Computing team supports modern biological research in several ways. They maintain a world-class computational infrastructure, including storage and backup infrastructure, a high-performance compute cluster, and all Linux systems. They also manage Janelia's data center and backup and disaster recovery resources. The team supports data storage infrastructure for storing and accessing scientific data, with over 15 petabytes of scientific data split across various storage tiers. They also support a Linux compute cluster with over 5000 cores and 300 GPUs, and maintain many other Linux servers and workstations. \\n\\nIn addition to infrastructure support, the Scientific Computing Software team works closely with Janelia's labs, project teams, and shared resources to help with research and engineering tasks. They provide everything from answering quick questions to full software life cycle support. The team's software skills span a broad range of programming languages, extendable applications, frameworks, cloud & cluster technologies, and databases. They also have deep domain knowledge in image processing, machine learning, data handling, electron and light microscopy, instrument control, 3D graphics & visualization, bioinformatics & transcriptomics. \\n\\nThe team also develops and maintains a variety of tools and projects, such as NeuronBridge, HortaCloud, VVD Viewer, EASI-FISH pipeline, Render, RS-FISH, and BigStitcher. They are strong proponents of open science and most of their software is open source and available via GitHub. They also run the Scientific Computing Associates program, which offers challenging assignments for those interested in computational science.\", \"The Janelia Scientific Computing team provides comprehensive support for advanced imaging techniques in neuroscience, cell biology, and bioinformatics through a variety of collaborations and custom software tools. They work closely with Janelia's labs, project teams, and shared resources to assist with research and engineering tasks. This can range from answering quick questions to providing full software life cycle support.\\n\\nThe team's software skills cover a broad range of programming languages, extendable applications, frameworks, cloud & cluster technologies, and databases. They have deep domain knowledge in image processing, machine learning, data handling, electron and light microscopy, instrument control, 3D graphics & visualization, bioinformatics & transcriptomics. Many team members have backgrounds in biology, enabling them to work efficiently with experimentalists and computer scientists.\\n\\nThe team is also involved in various projects and tools such as NeuronBridge, HortaCloud, VVD Viewer, EASI-FISH pipeline, Render, RS-FISH, and BigStitcher, which are designed to support advanced imaging techniques and data analysis in neuroscience, cell biology, and bioinformatics.\\n\\nFurthermore, the team is a strong proponent of open science and has teamed up with the Computation & Theory research area to create the Open Science Software Initiative. Most of their software is open source and available via GitHub, promoting collaboration and knowledge sharing. They also run the Scientific Computing Associates program, which embeds associates in SciComp and the lab or team they work with.\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9699a438",
   "metadata": {},
   "source": [
    "Add values from the list of JaneliaGPT responses to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eaea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['answer'] = None\n",
    "# Assuming df is your DataFrame and answers_list is a list with values to populate the 'Answer' column\n",
    "if len(df) == len(answers_list):\n",
    "    df['answer'] = answers_list\n",
    "else:\n",
    "    print(\"The length of answers_list does not match the number of rows in the DataFrame.\")\n",
    "\n",
    "df.dropna(axis=1, how='all', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a799ec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "display(df)\n",
    "df.to_json('WithAnswersDatasetFromTestTxt.json', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afdbad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempt to fix the issue with the answers_list not bsjdkflasdjjfklasdkl fasdjkfh jkas\n",
    "df_fix = df\n",
    "# List of columns to keep\n",
    "columns_to_keep = ['question', 'ground_truth', 'answer', 'contexts']\n",
    "\n",
    "# Reassign df to a DataFrame containing only the columns to keep\n",
    "df_fix= df_fix[columns_to_keep]\n",
    "\n",
    "display(df_fix['contexts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea358b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is already defined and contains the necessary columns\n",
    "\n",
    "# Convert 'question' and 'answer' to lists of strings if they are not already\n",
    "df_fix['question'] = df_fix['question'].apply(lambda x: [x] if isinstance(x, str) else x)\n",
    "df_fix['answer'] = df_fix['answer'].apply(lambda x: [x] if isinstance(x, str) else x)\n",
    "\n",
    "# Ensure 'contexts' and 'ground_truth' are lists of lists of strings\n",
    "# This step assumes 'contexts' and 'ground_truth' are already in the correct format\n",
    "# If not, you would need to apply a similar conversion as above, ensuring each element is a list\n",
    "\n",
    "# Example conversion if 'contexts' and 'ground_truth' were not already lists of lists\n",
    "df_fix['contexts'] = df_fix['contexts'].apply(lambda x: [[y] for y in x] if isinstance(x, list) and all(isinstance(y, str) for y in x) else x)\n",
    "df_fix['ground_truth'] = df_fix['ground_truth'].apply(lambda x: [[y] for y in x] if isinstance(x, list) and all(isinstance(y, str) for y in x) else x)\n",
    "\n",
    "# Now df should be in the correct format for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f1b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This converts to dictionary, dont think we need this right now\n",
    "\n",
    "# # Select the first row of the DataFrame (assuming it's the one you want to convert)\n",
    "# row = df_fix.iloc[0]\n",
    "\n",
    "# # Convert the selected row to a dictionary\n",
    "# result_dict = {\n",
    "#     'question': row['question'][0] if row['question'] else None,  # Assuming 'question' is a list of strings\n",
    "#     'answer': row['answer'][0] if row['answer'] else None,  # Assuming 'answer' is a list of strings\n",
    "#     'contexts': row['contexts'] if row['contexts'] else [],  # Assuming 'contexts' is already a list of strings\n",
    "#     'ground_truths': row['ground_truth'] if row['ground_truth'] else []  # Renaming key and assuming it's a list of lists\n",
    "# }\n",
    "# print (result_dict)\n",
    "# # result_dict now resembles the desired format\n",
    "\n",
    "# formatted_dataset = {\n",
    "#     'features': list(result_dict.keys()),  # List of keys from result_dict\n",
    "#     'num_rows': 1  # Since we're only dealing with a single row\n",
    "# }\n",
    "\n",
    "# print(formatted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f908cefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "amnesty_qa = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v2\")\n",
    "display (amnesty_qa[\"eval\"])\n",
    "display(df_fix)\n",
    "#  print (formatted_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c340b7d6",
   "metadata": {},
   "source": [
    "Run the evaluation given the dataframe (does not work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095ca61",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55166bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset_fix = Dataset.from_pandas(df_fix)\n",
    "dataset_fix = dataset_fix.remove_columns(['__index_level_0__'])\n",
    "\n",
    "print (dataset_fix)\n",
    "print (amnesty_qa[\"eval\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec82fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Features\n",
    "\n",
    "# Assuming dataset_fix and amnesty_qa[\"eval\"] are your datasets\n",
    "\n",
    "# Access the Features of each dataset\n",
    "features_dataset_fix = dataset_fix.features\n",
    "features_amnesty_eval = amnesty_qa[\"eval\"].features\n",
    "def format_columns(example):\n",
    "    # Format 'question', 'answer', and 'ground_truths' columns to single values\n",
    "    for column in ['question', 'answer', 'ground_truths']:\n",
    "        if column in example and example[column]:\n",
    "            example[column] = example[column][0]\n",
    "    \n",
    "    # Correctly format 'contexts' column to a list of list of strings\n",
    "    if 'contexts' in example:\n",
    "        # Ensure 'contexts' is a list of strings (not a list of lists)\n",
    "        if isinstance(example['contexts'], list):\n",
    "            # If the items are lists (or other non-string types), flatten and convert to strings\n",
    "            example['contexts'] = [str(item) for sublist in example['contexts'] for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "        else:\n",
    "            # If 'contexts' is not a list, convert it into a list of a single string\n",
    "            example['contexts'] = [str(example['contexts'])]\n",
    "\n",
    "    \n",
    "    return example\n",
    "\n",
    "\n",
    "# Apply the transformation to both datasets\n",
    "dataset_fix = dataset_fix.map(format_columns)\n",
    "# Direct comparison of data types\n",
    "if set(features_dataset_fix.keys()) == set(features_amnesty_eval.keys()):\n",
    "    all_types_match = True\n",
    "    for key in features_dataset_fix.keys():\n",
    "        type_dataset_fix = type(features_dataset_fix[key]).__name__\n",
    "        type_amnesty_eval = type(features_amnesty_eval[key]).__name__\n",
    "        if type_dataset_fix != type_amnesty_eval:\n",
    "            print(f\"Data type for feature '{key}' differs between datasets. dataset_fix: {type_dataset_fix}, amnesty_qa['eval']: {type_amnesty_eval}\")\n",
    "            all_types_match = False\n",
    "    if all_types_match:\n",
    "        print(\"The data types of all features in both datasets match.\")\n",
    "else:\n",
    "    print(\"The Features of the datasets differ in their keys.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be48f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "\n",
    "from ragas import evaluate\n",
    "\n",
    "\n",
    "# Will reutrn a dataframe with the metrics\n",
    "# Returns error for now because answers column is missing\n",
    "# Error is misleading, fix dataset first and make it match the docs example dataset\n",
    "\n",
    "result = evaluate(\n",
    "    dataset_fix,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "    ],\n",
    ")\n",
    "\n",
    "result\n",
    "\n",
    "result.to_pandas()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

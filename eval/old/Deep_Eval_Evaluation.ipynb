{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/rich/live.py:231: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/rich/live.py:231: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/rich/live.py:231: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/rich/live.py:231: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/rich/live.py:231: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/rich/live.py:231: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/rich/live.py:231: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/rich/live.py:231: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/rich/live.py:231: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/rich/live.py:231: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Golden(input='What are the key focus areas of the Janelia Scientific Computing team, including HPC and data management?', actual_output=None, expected_output=None, context=['Overview of Janelia Scientific Computing\\nThe Janelia Scientific Computing team supports the institute’s mission by developing and maintaining computational infrastructure, software tools, and data analysis pipelines essential for modern biological research. They collaborate closely with scientists to address computational challenges and drive innovation in various research areas, including neuroscience, cell biology, and bioinformatics.\\n\\nKey Areas of Focus\\nHigh-Performance Computing (HPC):\\n\\nProvides access to powerful computing clusters to handle large-scale data analysis and complex simulations.\\nOptimizes computational workflows to maximize efficiency and resource utilization.\\nData Management and Storage:\\n\\nDevelops and maintains robust data storage solutions to handle the vast amounts of data generated by research activities.\\nEnsures data integrity, security, and accessibility.\\nSoftware Development:\\n\\nCreates custom software tools and platforms tailored to specific research needs.\\nDevelops user-friendly interfaces and integrates with existing scientific tools to enhance usability.\\nBioinformatics and Data Analysis:\\n\\nProvides expertise in bioinformatics to analyze genomic, proteomic, and other biological data.\\nDevelops algorithms and pipelines for data processing, statistical analysis, and visualization.\\nImaging and Image Analysis:\\n\\nSupports advanced imaging techniques, including light microscopy, electron microscopy, and in vivo imaging.\\nDevelops image analysis tools to process and quantify complex imaging data.\\nCollaboration and Support:\\n\\nWorks closely with researchers to understand their computational needs and provide tailored solutions.\\nOffers training and support to scientists to enhance their computational skills.\\nNotable Projects and Contributions\\nFlyEM Project:\\n\\nThe Fly Electron Microscopy (FlyEM) project aims to map the entire brain of the fruit fly, Drosophila melanogaster, at nanoscale resolution using electron microscopy.\\nThe Scientific Computing team has developed sophisticated data processing pipelines and analysis tools to handle the massive datasets generated by this project.\\nNeuroglancer:\\n\\nAn open-source web-based tool for visualizing large-scale volumetric data, such as 3D electron microscopy images.\\nDeveloped in collaboration with Google, Neuroglancer allows researchers to interactively explore complex datasets in real-time.\\nJanelia Workstation:\\n\\nA powerful software platform for managing and analyzing microscopy data.\\nIntegrates various image processing and analysis tools into a cohesive workflow, streamlining the research process.\\nOpen-source Contributions:\\n\\nThe team actively contributes to the open-source community, releasing tools and software developed at Janelia for wider use in the scientific community.\\nExamples include the suite of tools for image analysis and neuroinformatics.\\nFacilities and Infrastructure\\nData Center: Houses high-performance computing clusters, massive storage systems, and networking infrastructure essential for supporting computational research.\\nCollaboration Spaces: Equipped with state-of-the-art visualization and collaboration tools to facilitate teamwork and knowledge sharing among scientists and computational experts.\\nEducational and Training Initiatives\\nWorkshops and Training Sessions: Regularly organizes workshops and training sessions to educate researchers on computational tools, data analysis techniques, and best practices.\\nDocumentation and Resources: Provides comprehensive documentation, tutorials, and online resources to help researchers effectively use computational tools and resources.\\nImpact on Research\\nThe Janelia Scientific Computing team’s contributions have significantly impacted various research areas:\\n\\nNeuroscience: Enabled groundbreaking discoveries in understanding brain function and neural circuits through advanced data analysis and visualization tools.\\nCell Biology: Facilitated high-throughput data analysis and imaging techniques, leading to new insights into cellular processes and structures.\\nGenomics: Supported large-scale genomic studies with robust bioinformatics tools, accelerating discoveries in genetics and molecular biology.\\nLeadership and Team Composition\\nLeadership: The team is led by experts in computational science and bioinformatics, who bring a wealth of experience and vision to the team.\\nTeam Composition: Comprises computational scientists, software developers, data analysts, and bioinformatics specialists, each bringing unique skills and expertise to the team.\\nFuture Directions\\nThe Janelia Scientific Computing team is continually evolving to meet the growing computational demands of modern biological research. Future directions include:\\n\\nIntegration of AI and Machine Learning: Leveraging AI and machine learning techniques to enhance data analysis and interpretation.\\nExpansion of Cloud Computing: Exploring cloud computing solutions to provide scalable and flexible computational resources.\\nDevelopment of Next-Generation Tools: Continuing to innovate and develop new tools and platforms to support emerging research needs.\\nConclusion\\nThe Janelia Scientific Computing team is an integral part of the research ecosystem at Janelia Research Campus, providing essential computational support and driving innovation across various scientific disciplines. Their work enables researchers to tackle complex biological questions and make significant scientific advancements.\\n\\n\\n\\n\\n\\n\\n'], retrieval_context=None, additional_metadata=None, comments=None, source_file='test.txt'),\n",
       " Golden(input='How does the Janelia Scientific Computing team enhance neuroscience, cell biology, and bioinformatics research through their computational infrastructure, software tools, and data analysis pipelines?', actual_output=None, expected_output=None, context=['Overview of Janelia Scientific Computing\\nThe Janelia Scientific Computing team supports the institute’s mission by developing and maintaining computational infrastructure, software tools, and data analysis pipelines essential for modern biological research. They collaborate closely with scientists to address computational challenges and drive innovation in various research areas, including neuroscience, cell biology, and bioinformatics.\\n\\nKey Areas of Focus\\nHigh-Performance Computing (HPC):\\n\\nProvides access to powerful computing clusters to handle large-scale data analysis and complex simulations.\\nOptimizes computational workflows to maximize efficiency and resource utilization.\\nData Management and Storage:\\n\\nDevelops and maintains robust data storage solutions to handle the vast amounts of data generated by research activities.\\nEnsures data integrity, security, and accessibility.\\nSoftware Development:\\n\\nCreates custom software tools and platforms tailored to specific research needs.\\nDevelops user-friendly interfaces and integrates with existing scientific tools to enhance usability.\\nBioinformatics and Data Analysis:\\n\\nProvides expertise in bioinformatics to analyze genomic, proteomic, and other biological data.\\nDevelops algorithms and pipelines for data processing, statistical analysis, and visualization.\\nImaging and Image Analysis:\\n\\nSupports advanced imaging techniques, including light microscopy, electron microscopy, and in vivo imaging.\\nDevelops image analysis tools to process and quantify complex imaging data.\\nCollaboration and Support:\\n\\nWorks closely with researchers to understand their computational needs and provide tailored solutions.\\nOffers training and support to scientists to enhance their computational skills.\\nNotable Projects and Contributions\\nFlyEM Project:\\n\\nThe Fly Electron Microscopy (FlyEM) project aims to map the entire brain of the fruit fly, Drosophila melanogaster, at nanoscale resolution using electron microscopy.\\nThe Scientific Computing team has developed sophisticated data processing pipelines and analysis tools to handle the massive datasets generated by this project.\\nNeuroglancer:\\n\\nAn open-source web-based tool for visualizing large-scale volumetric data, such as 3D electron microscopy images.\\nDeveloped in collaboration with Google, Neuroglancer allows researchers to interactively explore complex datasets in real-time.\\nJanelia Workstation:\\n\\nA powerful software platform for managing and analyzing microscopy data.\\nIntegrates various image processing and analysis tools into a cohesive workflow, streamlining the research process.\\nOpen-source Contributions:\\n\\nThe team actively contributes to the open-source community, releasing tools and software developed at Janelia for wider use in the scientific community.\\nExamples include the suite of tools for image analysis and neuroinformatics.\\nFacilities and Infrastructure\\nData Center: Houses high-performance computing clusters, massive storage systems, and networking infrastructure essential for supporting computational research.\\nCollaboration Spaces: Equipped with state-of-the-art visualization and collaboration tools to facilitate teamwork and knowledge sharing among scientists and computational experts.\\nEducational and Training Initiatives\\nWorkshops and Training Sessions: Regularly organizes workshops and training sessions to educate researchers on computational tools, data analysis techniques, and best practices.\\nDocumentation and Resources: Provides comprehensive documentation, tutorials, and online resources to help researchers effectively use computational tools and resources.\\nImpact on Research\\nThe Janelia Scientific Computing team’s contributions have significantly impacted various research areas:\\n\\nNeuroscience: Enabled groundbreaking discoveries in understanding brain function and neural circuits through advanced data analysis and visualization tools.\\nCell Biology: Facilitated high-throughput data analysis and imaging techniques, leading to new insights into cellular processes and structures.\\nGenomics: Supported large-scale genomic studies with robust bioinformatics tools, accelerating discoveries in genetics and molecular biology.\\nLeadership and Team Composition\\nLeadership: The team is led by experts in computational science and bioinformatics, who bring a wealth of experience and vision to the team.\\nTeam Composition: Comprises computational scientists, software developers, data analysts, and bioinformatics specialists, each bringing unique skills and expertise to the team.\\nFuture Directions\\nThe Janelia Scientific Computing team is continually evolving to meet the growing computational demands of modern biological research. Future directions include:\\n\\nIntegration of AI and Machine Learning: Leveraging AI and machine learning techniques to enhance data analysis and interpretation.\\nExpansion of Cloud Computing: Exploring cloud computing solutions to provide scalable and flexible computational resources.\\nDevelopment of Next-Generation Tools: Continuing to innovate and develop new tools and platforms to support emerging research needs.\\nConclusion\\nThe Janelia Scientific Computing team is an integral part of the research ecosystem at Janelia Research Campus, providing essential computational support and driving innovation across various scientific disciplines. Their work enables researchers to tackle complex biological questions and make significant scientific advancements.\\n\\n\\n\\n\\n\\n\\n'], retrieval_context=None, additional_metadata=None, comments=None, source_file='test.txt')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval.synthesizer import Synthesizer\n",
    "\n",
    "synthesizer = Synthesizer()\n",
    "synthesizer.generate_goldens_from_docs(\n",
    "    document_paths=['test.txt'],\n",
    "    max_goldens_per_document=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/rich/live.py:231: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/bakhalea/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/rich/live.py:231: UserWarning: \n",
       "install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test cases...\n",
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Synthesizer' object has no attribute 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepeval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtest_case\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMTestCase\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepeval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[0;32m---> 17\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msynthesizer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcontextual_precision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontextual_recall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontextual_relevancy\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/deepeval/evaluate.py:458\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(test_cases, metrics, hyperparameters, run_async, show_indicator, print_results, write_cache, use_cache, ignore_errors)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_async:\n\u001b[1;32m    457\u001b[0m     loop \u001b[38;5;241m=\u001b[39m get_or_create_event_loop()\n\u001b[0;32m--> 458\u001b[0m     test_results \u001b[38;5;241m=\u001b[39m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43ma_execute_test_cases\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m            \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m            \u001b[49m\u001b[43msave_to_disk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    468\u001b[0m     test_results \u001b[38;5;241m=\u001b[39m execute_test_cases(\n\u001b[1;32m    469\u001b[0m         test_cases,\n\u001b[1;32m    470\u001b[0m         metrics,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m         save_to_disk\u001b[38;5;241m=\u001b[39mwrite_cache,\n\u001b[1;32m    474\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py:316\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    314\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/deepeval/evaluate.py:306\u001b[0m, in \u001b[0;36ma_execute_test_cases\u001b[0;34m(test_cases, metrics, ignore_errors, use_cache, save_to_disk)\u001b[0m\n\u001b[1;32m    304\u001b[0m new_cached_test_case: CachedTestCase \u001b[38;5;241m=\u001b[39m CachedTestCase()\n\u001b[1;32m    305\u001b[0m test_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m measure_metrics_with_indicator(\n\u001b[1;32m    307\u001b[0m     metrics, test_case, cached_test_case, ignore_errors\n\u001b[1;32m    308\u001b[0m )\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[1;32m    311\u001b[0m     metric_metadata \u001b[38;5;241m=\u001b[39m create_metric_metadata(metric)\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/deepeval/metrics/indicator.py:150\u001b[0m, in \u001b[0;36mmeasure_metrics_with_indicator\u001b[0;34m(metrics, test_case, cached_test_case, ignore_errors)\u001b[0m\n\u001b[1;32m    134\u001b[0m             task_id \u001b[38;5;241m=\u001b[39m progress\u001b[38;5;241m.\u001b[39madd_task(\n\u001b[1;32m    135\u001b[0m                 description\u001b[38;5;241m=\u001b[39mformat_metric_description(\n\u001b[1;32m    136\u001b[0m                     metric, async_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    137\u001b[0m                 ),\n\u001b[1;32m    138\u001b[0m                 total\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m    139\u001b[0m             )\n\u001b[1;32m    140\u001b[0m             tasks\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    141\u001b[0m                 measure_metric_task(\n\u001b[1;32m    142\u001b[0m                     task_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 )\n\u001b[1;32m    149\u001b[0m             )\n\u001b[0;32m--> 150\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py:385\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 385\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py:314\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/deepeval/metrics/indicator.py:89\u001b[0m, in \u001b[0;36mmeasure_metric_task\u001b[0;34m(task_id, progress, metric, test_case, cached_test_case, ignore_errors)\u001b[0m\n\u001b[1;32m     86\u001b[0m     tc \u001b[38;5;241m=\u001b[39m test_case\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m metric\u001b[38;5;241m.\u001b[39ma_measure(tc, _show_indicator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     90\u001b[0m     finish_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/deepeval/metrics/contextual_precision/contextual_precision.py:98\u001b[0m, in \u001b[0;36mContextualPrecisionMetric.a_measure\u001b[0;34m(self, test_case, _show_indicator)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(test_case, ConversationalTestCase):\n\u001b[1;32m     97\u001b[0m     test_case \u001b[38;5;241m=\u001b[39m validate_conversational_test_case(test_case, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[43mcheck_llm_test_case_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequired_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musing_native_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m metric_progress_indicator(\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    103\u001b[0m     async_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    104\u001b[0m     _show_indicator\u001b[38;5;241m=\u001b[39m_show_indicator,\n\u001b[1;32m    105\u001b[0m ):\n",
      "File \u001b[0;32m~/Documents/gpt-semantic-search/env/lib/python3.12/site-packages/deepeval/metrics/utils.py:32\u001b[0m, in \u001b[0;36mcheck_llm_test_case_params\u001b[0;34m(test_case, test_case_params, metric)\u001b[0m\n\u001b[1;32m     30\u001b[0m missing_params \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m test_case_params:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m         missing_params\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_params:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Synthesizer' object has no attribute 'input'"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric\n",
    ")\n",
    "\n",
    "contextual_precision = ContextualPrecisionMetric()\n",
    "contextual_recall = ContextualRecallMetric()\n",
    "contextual_relevancy = ContextualRelevancyMetric()\n",
    "\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval import evaluate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "evaluate(\n",
    "    test_cases=[synthesizer],\n",
    "    metrics=[contextual_precision, contextual_recall, contextual_relevancy]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

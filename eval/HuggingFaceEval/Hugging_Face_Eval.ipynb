{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Configuration\"\"\"\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test Responses Huggingface\"\"\"\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Use your personal token here\n",
    "TOKEN = \"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\"\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-xxl\"\n",
    "headers = {\"Authorization\": f\"Bearer {TOKEN}\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "    \n",
    "output = query({\n",
    "    \"inputs\": \"The answer to the universe is\",\n",
    "})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b9cf1b6b704e56a3b785e19d72026c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13841\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "langchain_docs = [LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(ds)]\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "print(len(docs_processed))\n",
    "with open(\"docs_processed.txt\", \"w\") as f:\n",
    "    for doc in docs_processed:\n",
    "        f.write((str(doc)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like we're just getting started with this conversation. I'm here to help, so feel free to ask me any questions or share what's on your mind. What would you like to talk about?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "# def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "#     response = inference_client.post(\n",
    "#         json={\n",
    "#             \"inputs\": prompt,\n",
    "#             \"parameters\": {\"max_new_tokens\": 1000},\n",
    "#             \"task\": \"text-generation\",\n",
    "#         },\n",
    "#     )\n",
    "#     return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "\n",
    "# Load LLaMA model and tokenizer\n",
    "\n",
    "def call_llm(prompt: str):\n",
    "    # Initialize the Ollama model\n",
    "    llm = Ollama(model=\"llama3\")  # You can change \"llama2\" to the specific model you want to use\n",
    "    \n",
    "    # Generate text using the model\n",
    "    response = llm(prompt)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "result = call_llm(\"This is a test context\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prompts\"\"\"\n",
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\"\n",
    "\n",
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2 QA couples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:03<00:03,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA Gen Prompt:  \n",
      "Your task is to write a factoid question and an answer given a context.\n",
      "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
      "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
      "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
      "\n",
      "Provide your answer as follows:\n",
      "\n",
      "Output:::\n",
      "Factoid question: (your factoid question)\n",
      "Answer: (your answer to the factoid question)\n",
      "\n",
      "Now here is the context.\n",
      "\n",
      "Context: Start by loading the model and associated processor from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?other=owlvit).\n",
      "Here we'll use the same checkpoint as before:\n",
      "\n",
      "```py\n",
      ">>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
      "\n",
      ">>> model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint)\n",
      ">>> processor = AutoProcessor.from_pretrained(checkpoint)\n",
      "```\n",
      "\n",
      "Let's take a different image to switch things up.\n",
      "\n",
      "```py\n",
      ">>> import requests\n",
      "\n",
      ">>> url = \"https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&force=true&w=640\"\n",
      ">>> im = Image.open(requests.get(url, stream=True).raw)\n",
      ">>> im\n",
      "```\n",
      "\n",
      "<div class=\"flex justify-center\">\n",
      "     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_3.png\" alt=\"Beach photo\"/>\n",
      "</div>\n",
      "\n",
      "Use the processor to prepare the inputs for the model. The processor combines an image processor that prepares the\n",
      "image for the model by resizing and normalizing it, and a [`CLIPTokenizer`] that takes care of the text inputs.\n",
      "\n",
      "```py\n",
      ">>> text_queries = [\"hat\", \"book\", \"sunglasses\", \"camera\"]\n",
      ">>> inputs = processor(text=text_queries, images=im, return_tensors=\"pt\")\n",
      "```\n",
      "\n",
      "Pass the inputs through the model, post-process, and visualize the results. Since the image processor resized images before\n",
      "feeding them to the model, you need to use the [`~OwlViTImageProcessor.post_process_object_detection`] method to make sure the predicted bounding\n",
      "boxes have the correct coordinates relative to the original image:\n",
      "\n",
      "```py\n",
      ">>> import torch\n",
      "\n",
      ">>> with torch.no_grad():\n",
      "...     outputs = model(**inputs)\n",
      "...     target_sizes = torch.tensor([im.size[::-1]])\n",
      "...     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)[0]\n",
      "\n",
      ">>> draw = ImageDraw.Draw(im)\n",
      "\n",
      "Output:::\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA Gen Prompt:  \n",
      "Your task is to write a factoid question and an answer given a context.\n",
      "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
      "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
      "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
      "\n",
      "Provide your answer as follows:\n",
      "\n",
      "Output:::\n",
      "Factoid question: (your factoid question)\n",
      "Answer: (your answer to the factoid question)\n",
      "\n",
      "Now here is the context.\n",
      "\n",
      "Context: Gradio Demo: blocks_component_shortcut\n",
      "\n",
      "\n",
      "```\n",
      "!pip install -q gradio \n",
      "```\n",
      "\n",
      "\n",
      "```\n",
      "import gradio as gr\n",
      "\n",
      "\n",
      "def greet(str):\n",
      "    return str\n",
      "\n",
      "\n",
      "with gr.Blocks() as demo:\n",
      "    \"\"\"\n",
      "    You can make use of str shortcuts you use in Interface within Blocks as well.\n",
      "    \n",
      "    Interface shortcut example:\n",
      "    Interface(greet, \"textarea\", \"textarea\")\n",
      "    \n",
      "    You can use \n",
      "    1. gr.component()\n",
      "    2. gr.templates.Template()\n",
      "    3. gr.Template()\n",
      "    All the templates are listed in gradio/templates.py\n",
      "    \"\"\"\n",
      "    with gr.Row():\n",
      "            text1 = gr.component(\"textarea\")\n",
      "            text2 = gr.TextArea()\n",
      "            text3 = gr.templates.TextArea()\n",
      "    text1.blur(greet, text1, text2)\n",
      "    text2.blur(greet, text2, text3)\n",
      "    text3.blur(greet, text3, text1)\n",
      "    button = gr.component(\"button\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    demo.launch()\n",
      "\n",
      "```\n",
      "\n",
      "Output:::\n",
      "Generated 2 QA couples successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "N_GENERATIONS = 2  # We intentionally generate only 10 QA couples here for cost and time considerations\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "\n",
    "def call_llm(prompt: str):\n",
    "    # Initialize the Ollama model\n",
    "    llm = Ollama(model=\"llama3\")  # You can change \"llama2\" to the specific model you want to use\n",
    "    \n",
    "    # Generate text using the model\n",
    "    response = llm(prompt)\n",
    "    \n",
    "    return response\n",
    "\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    qa_prompt = QA_generation_prompt.format(context=sampled_context.page_content)\n",
    "    output_QA_couple = call_llm(qa_prompt)\n",
    "    print(\"QA Gen Prompt: \", qa_prompt)\n",
    "    try:\n",
    "        # Extract the \"Output:::\" part\n",
    "        output_section = output_QA_couple.split(\"Output:::\")[-1].strip()\n",
    "        \n",
    "        # Extract question and answer\n",
    "        question = output_section.split(\"Factoid question:\")[-1].split(\"Answer:\")[0].strip()\n",
    "        answer = output_section.split(\"Answer:\")[-1].strip()\n",
    "        \n",
    "        assert len(answer) < 300, \"Answer is too long\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing QA couple: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "print(f\"Generated {len(outputs)} QA couples successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Start by loading the model and associated processor from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?other=owlvit).\\nHere we'll use the same checkpoint as before:\\n\\n```py\\n&gt;&gt;&gt; from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\\n\\n&gt;&gt;&gt; model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint)\\n&gt;&gt;&gt; processor = AutoProcessor.from_pretrained(checkpoint)\\n```\\n\\nLet's take a different image to switch things up.\\n\\n```py\\n&gt;&gt;&gt; import requests\\n\\n&gt;&gt;&gt; url = \"https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&amp;force=true&amp;w=640\"\\n&gt;&gt;&gt; im = Image.open(requests.get(url, stream=True).raw)\\n&gt;&gt;&gt; im\\n```\\n\\n&lt;div class=\"flex justify-center\"&gt;\\n     &lt;img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_3.png\" alt=\"Beach photo\"/&gt;\\n&lt;/div&gt;\\n\\nUse the processor to prepare the inputs for the model. The processor combines an image processor that prepares the\\nimage for the model by resizing and normalizing it, and a [`CLIPTokenizer`] that takes care of the text inputs.\\n\\n```py\\n&gt;&gt;&gt; text_queries = [\"hat\", \"book\", \"sunglasses\", \"camera\"]\\n&gt;&gt;&gt; inputs = processor(text=text_queries, images=im, return_tensors=\"pt\")\\n```\\n\\nPass the inputs through the model, post-process, and visualize the results. Since the image processor resized images before\\nfeeding them to the model, you need to use the [`~OwlViTImageProcessor.post_process_object_detection`] method to make sure the predicted bounding\\nboxes have the correct coordinates relative to the original image:\\n\\n```py\\n&gt;&gt;&gt; import torch\\n\\n&gt;&gt;&gt; with torch.no_grad():\\n...     outputs = model(**inputs)\\n...     target_sizes = torch.tensor([im.size[::-1]])\\n...     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)[0]\\n\\n&gt;&gt;&gt; draw = ImageDraw.Draw(im)</td>\n",
       "      <td>What is the URL of the image used in this example?</td>\n",
       "      <td>https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&amp;force=true&amp;w=640</td>\n",
       "      <td>huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   context  \\\n",
       "0  Start by loading the model and associated processor from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?other=owlvit).\\nHere we'll use the same checkpoint as before:\\n\\n```py\\n>>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\\n\\n>>> model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint)\\n>>> processor = AutoProcessor.from_pretrained(checkpoint)\\n```\\n\\nLet's take a different image to switch things up.\\n\\n```py\\n>>> import requests\\n\\n>>> url = \"https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&force=true&w=640\"\\n>>> im = Image.open(requests.get(url, stream=True).raw)\\n>>> im\\n```\\n\\n<div class=\"flex justify-center\">\\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_3.png\" alt=\"Beach photo\"/>\\n</div>\\n\\nUse the processor to prepare the inputs for the model. The processor combines an image processor that prepares the\\nimage for the model by resizing and normalizing it, and a [`CLIPTokenizer`] that takes care of the text inputs.\\n\\n```py\\n>>> text_queries = [\"hat\", \"book\", \"sunglasses\", \"camera\"]\\n>>> inputs = processor(text=text_queries, images=im, return_tensors=\"pt\")\\n```\\n\\nPass the inputs through the model, post-process, and visualize the results. Since the image processor resized images before\\nfeeding them to the model, you need to use the [`~OwlViTImageProcessor.post_process_object_detection`] method to make sure the predicted bounding\\nboxes have the correct coordinates relative to the original image:\\n\\n```py\\n>>> import torch\\n\\n>>> with torch.no_grad():\\n...     outputs = model(**inputs)\\n...     target_sizes = torch.tensor([im.size[::-1]])\\n...     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)[0]\\n\\n>>> draw = ImageDraw.Draw(im)   \n",
       "\n",
       "                                             question  \\\n",
       "0  What is the URL of the image used in this example?   \n",
       "\n",
       "                                                                                                                                   answer  \\\n",
       "0  https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&force=true&w=640   \n",
       "\n",
       "                                                                              source_doc  \n",
       "0  huggingface/transformers/blob/main/docs/source/en/tasks/zero_shot_object_detection.md  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(outputs).head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating critique for each QA couple...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:12<00:00,  6.35s/it]\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "for output in tqdm(outputs):\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(\n",
    "            question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]),\n",
    "        ),\n",
    "        \"relevance\": call_llm(\n",
    "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "        \"standalone\": call_llm(\n",
    "            question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "    }\n",
    "    try:\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            score, eval = (\n",
    "                int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
    "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
    "            )\n",
    "            output.update(\n",
    "                {\n",
    "                    f\"{criterion}_score\": score,\n",
    "                    f\"{criterion}_eval\": eval,\n",
    "                }\n",
    "            )\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset before filtering:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the URL of the image used in this example?</td>\n",
       "      <td>https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&amp;force=true&amp;w=640</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the name of the library installed at the beginning of the code?</td>\n",
       "      <td>Gradio</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  question  \\\n",
       "0                       What is the URL of the image used in this example?   \n",
       "1  What is the name of the library installed at the beginning of the code?   \n",
       "\n",
       "                                                                                                                                   answer  \\\n",
       "0  https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&force=true&w=640   \n",
       "1                                                                                                                                  Gradio   \n",
       "\n",
       "   groundedness_score  relevance_score  standalone_score  \n",
       "0                   1                2                 5  \n",
       "1                   5                1                 5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "Final evaluation dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [question, answer, groundedness_score, relevance_score, standalone_score]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "\n",
    "print(\"Evaluation dataset before filtering:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "generated_questions = generated_questions.loc[\n",
    "    (generated_questions[\"groundedness_score\"] >= 4)\n",
    "    & (generated_questions[\"relevance_score\"] >= 4)\n",
    "    & (generated_questions[\"standalone_score\"] >= 4)\n",
    "]\n",
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(generated_questions, split=\"train\", preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0f37982f1e4c88a3418495db8ab698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/893 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37077746a6b84e79908b00250ff1d1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/289k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fee89e8e7234e88aeb3d4136c67a28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/65 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "class_prefix is deprecated, please use index_name\n",
      "class_prefix is deprecated, please use index_name\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m service_src \u001b[38;5;241m=\u001b[39m SemanticSearchServiceSources(weaviate_url)\n\u001b[1;32m      9\u001b[0m service_ans \u001b[38;5;241m=\u001b[39m SemanticSearchService(weaviate_url)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_rag_tests\u001b[39m(\n\u001b[0;32m---> 13\u001b[0m     eval_dataset: \u001b[43mdatasets\u001b[49m\u001b[38;5;241m.\u001b[39mDataset,\n\u001b[1;32m     14\u001b[0m     llm,\n\u001b[1;32m     15\u001b[0m     knowledge_index: VectorStore,\n\u001b[1;32m     16\u001b[0m     output_file: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     17\u001b[0m     reranker: Optional[RAGPretrainedModel] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m     verbose: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m     test_settings: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# To document the test settings used\u001b[39;00m\n\u001b[1;32m     20\u001b[0m ):\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# load previous generations if they exist\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.language_models import BaseChatModel\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "from generate_answer import SemanticSearchService\n",
    "from generate_source import SemanticSearchServiceSources\n",
    "weaviate_url = \"http://localhost:8777\"\n",
    "service_src = SemanticSearchServiceSources(weaviate_url)\n",
    "service_ans = SemanticSearchService(weaviate_url)\n",
    "\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer = service_ans.generate_response(question)\n",
    "\n",
    "        relevant_docs = service_src.generate_response(question)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Configuration\"\"\"\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Authorization header is correct, but the token seems invalid'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test Responses Huggingface\"\"\"\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Use your personal token here\n",
    "TOKEN = \"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\"\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-xxl\"\n",
    "headers = {\"Authorization\": f\"Bearer {TOKEN}\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "    \n",
    "output = query({\n",
    "    \"inputs\": \"The answer to the universe is\",\n",
    "})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaba83c5d8d84675b7385ca2128b1acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13841\n"
     ]
    }
   ],
   "source": [
    "\"\"\"process example documents\"\"\"\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "langchain_docs = [LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(ds)]\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "print(len(docs_processed))\n",
    "with open(\"docs_processed.txt\", \"w\") as f:\n",
    "    for doc in docs_processed:\n",
    "        f.write((str(doc)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like this is indeed a test context. I'm happy to help with any questions or topics you'd like to discuss. What's on your mind?\n"
     ]
    }
   ],
   "source": [
    "\"\"\"test ollama call_llm\"\"\"\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "# def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "#     response = inference_client.post(\n",
    "#         json={\n",
    "#             \"inputs\": prompt,\n",
    "#             \"parameters\": {\"max_new_tokens\": 1000},\n",
    "#             \"task\": \"text-generation\",\n",
    "#         },\n",
    "#     )\n",
    "#     return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "\n",
    "# Load LLaMA model and tokenizer\n",
    "\n",
    "def call_llm(prompt: str):\n",
    "    # Initialize the Ollama model\n",
    "    llm = Ollama(model=\"llama3\")  # You can change \"llama2\" to the specific model you want to use\n",
    "    \n",
    "    # Generate text using the model\n",
    "    response = llm(prompt)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "result = call_llm(\"This is a test context\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prompts\"\"\"\n",
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\"\n",
    "\n",
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2 QA couples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:02<00:02,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA Gen Prompt:  \n",
      "Your task is to write a factoid question and an answer given a context.\n",
      "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
      "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
      "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
      "\n",
      "Provide your answer as follows:\n",
      "\n",
      "Output:::\n",
      "Factoid question: (your factoid question)\n",
      "Answer: (your answer to the factoid question)\n",
      "\n",
      "Now here is the context.\n",
      "\n",
      "Context: <!--\n",
      "Type: model-index\n",
      "Collections:\n",
      "- Name: TF EfficientNet CondConv\n",
      "  Paper:\n",
      "    Title: 'CondConv: Conditionally Parameterized Convolutions for Efficient Inference'\n",
      "    URL: https://paperswithcode.com/paper/soft-conditional-computation\n",
      "Models:\n",
      "- Name: tf_efficientnet_cc_b0_4e\n",
      "  In Collection: TF EfficientNet CondConv\n",
      "  Metadata:\n",
      "    FLOPs: 224153788\n",
      "    Parameters: 13310000\n",
      "    File Size: 53490940\n",
      "    Architecture:\n",
      "    - 1x1 Convolution\n",
      "    - Average Pooling\n",
      "    - Batch Normalization\n",
      "    - CondConv\n",
      "    - Convolution\n",
      "    - Dense Connections\n",
      "    - Dropout\n",
      "    - Inverted Residual Block\n",
      "    - Squeeze-and-Excitation Block\n",
      "    - Swish\n",
      "    Tasks:\n",
      "    - Image Classification\n",
      "    Training Techniques:\n",
      "    - AutoAugment\n",
      "    - Label Smoothing\n",
      "    - RMSProp\n",
      "    - Stochastic Depth\n",
      "    - Weight Decay\n",
      "    Training Data:\n",
      "    - ImageNet\n",
      "    ID: tf_efficientnet_cc_b0_4e\n",
      "    LR: 0.256\n",
      "    Epochs: 350\n",
      "    Crop Pct: '0.875'\n",
      "    Momentum: 0.9\n",
      "    Batch Size: 2048\n",
      "    Image Size: '224'\n",
      "    Weight Decay: 1.0e-05\n",
      "    Interpolation: bicubic\n",
      "    RMSProp Decay: 0.9\n",
      "    Label Smoothing: 0.1\n",
      "    BatchNorm Momentum: 0.99\n",
      "  Code: https://github.com/rwightman/pytorch-image-models/blob/9a25fdf3ad0414b4d66da443fe60ae0aa14edc84/timm/models/efficientnet.py#L1561\n",
      "  Weights: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_cc_b0_4e-4362b6b2.pth\n",
      "  Results:\n",
      "  - Task: Image Classification\n",
      "    Dataset: ImageNet\n",
      "    Metrics:\n",
      "      Top 1 Accuracy: 77.32%\n",
      "      Top 5 Accuracy: 93.32%\n",
      "- Name: tf_efficientnet_cc_b0_8e\n",
      "  In Collection: TF EfficientNet CondConv\n",
      "  Metadata:\n",
      "    FLOPs: 224158524\n",
      "    Parameters: 24010000\n",
      "    File Size: 96287616\n",
      "    Architecture:\n",
      "    - 1x1 Convolution\n",
      "    - Average Pooling\n",
      "    - Batch Normalization\n",
      "    - CondConv\n",
      "    - Convolution\n",
      "    - Dense Connections\n",
      "    - Dropout\n",
      "    - Inverted Residual Block\n",
      "    - Squeeze-and-Excitation Block\n",
      "    - Swish\n",
      "    Tasks:\n",
      "    - Image Classification\n",
      "    Training Techniques:\n",
      "    - AutoAugment\n",
      "\n",
      "Output:::\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA Gen Prompt:  \n",
      "Your task is to write a factoid question and an answer given a context.\n",
      "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
      "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
      "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
      "\n",
      "Provide your answer as follows:\n",
      "\n",
      "Output:::\n",
      "Factoid question: (your factoid question)\n",
      "Answer: (your answer to the factoid question)\n",
      "\n",
      "Now here is the context.\n",
      "\n",
      "Context: ```python\n",
      "def import_dataframe(project_id:str, dataset:pd.DataFrame, text_data_column:str, external_id_column:str, subset_size:int=100) -> bool:\n",
      "    \"\"\"\n",
      "    Arguments:\n",
      "    Inputs\n",
      "        - project_id (str): specifies the project to load the data, this is also returned when we create our project\n",
      "        - dataset (pandas DataFrame): Dataset that has proper columns for id and text inputs\n",
      "        - text_data_column (str): specifies which column has the text input data\n",
      "        - external_id_column (str): specifies which column has the ids\n",
      "        - subset_size (int): specifies the number of samples to import at a time. Cannot be higher than 100\n",
      "    \n",
      "    Outputs:\n",
      "        None\n",
      "    \n",
      "    Returns: \n",
      "        True or False regards to process succession\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    assert subset_size <= 100, \"Kili only allows to upload 100 assets at most at a time onto the app\"\n",
      "\n",
      "\n",
      "    L = len(dataset)\n",
      "\n",
      "    # set 25000 as an upload limit, can be changed\n",
      "    if L>25000:\n",
      "        print('Kili Projects currently supports maximum 25000 samples as default. Importing first 25000 samples...')\n",
      "        L=25000\n",
      "\n",
      "    i = 0\n",
      "\n",
      "    while i+subset_size < L:\n",
      "        \n",
      "        subset = dataset.iloc[i:i+subset_size]\n",
      "\n",
      "        externalIds = subset[external_id_column].astype(str).to_list()\n",
      "        contents = subset[text_data_column].astype(str).to_list()\n",
      "        \n",
      "        kili.append_many_to_dataset(project_id=project_id,\n",
      "                                    content_array=contents,\n",
      "                                    external_id_array=externalIds)\n",
      "\n",
      "        i += subset_size\n",
      "\n",
      "    return True\n",
      "```\n",
      "\n",
      "It simply imports the given `dataset` DataFrame to a project specified by project_id.\n",
      "\n",
      "We can see the arguments from docstring, we just need to pass our dataset along with the corresponding column names. We’ll just use the sample indices we get when we load the data. And then voila, uploading the data is done!\n",
      "\n",
      "Output:::\n",
      "Generated 2 QA couples successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"generate testset\"\"\"\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "N_GENERATIONS = 2  # We intentionally generate only 10 QA couples here for cost and time considerations\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "\n",
    "def call_llm(prompt: str):\n",
    "    # Initialize the Ollama model\n",
    "    llm = Ollama(model=\"llama3\")  # You can change \"llama2\" to the specific model you want to use\n",
    "    \n",
    "    # Generate text using the model\n",
    "    response = llm(prompt)\n",
    "    \n",
    "    return response\n",
    "\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    qa_prompt = QA_generation_prompt.format(context=sampled_context.page_content)\n",
    "    output_QA_couple = call_llm(qa_prompt)\n",
    "    print(\"QA Gen Prompt: \", qa_prompt)\n",
    "    try:\n",
    "        # Extract the \"Output:::\" part\n",
    "        output_section = output_QA_couple.split(\"Output:::\")[-1].strip()\n",
    "        \n",
    "        # Extract question and answer\n",
    "        question = output_section.split(\"Factoid question:\")[-1].split(\"Answer:\")[0].strip()\n",
    "        answer = output_section.split(\"Answer:\")[-1].strip()\n",
    "        \n",
    "        assert len(answer) < 300, \"Answer is too long\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing QA couple: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "print(f\"Generated {len(outputs)} QA couples successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If we go back to the calculator example, the following code will create the interface embedded below it.\\n\\n```python\\niface = gr.Interface(\\n    calculator,\\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\\n    \"number\",\\n    allow_flagging=\"manual\",\\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"]\\n)\\n\\niface.launch()\\n```\\n\\n&lt;gradio-app space=\"gradio/calculator-flagging-options/\"&gt;&lt;/gradio-app&gt;\\n\\nWhen users click the flag button, the csv file will now include a column indicating the selected option.\\n\\n_flagged/logs.csv_\\n\\n```csv\\nnum1,operation,num2,Output,flag,timestamp\\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\\n```\\n\\n## The HuggingFaceDatasetSaver Callback\\n\\nSometimes, saving the data to a local CSV file doesn't make sense. For example, on Hugging Face\\nSpaces, developers typically don't have access to the underlying ephemeral machine hosting the Gradio\\ndemo. That's why, by default, flagging is turned off in Hugging Face Space. However,\\nyou may want to do something else with the flagged data.\\n\\nWe've made this super easy with the `flagging_callback` parameter.\\n\\nFor example, below we're going to pipe flagged data from our calculator example into a Hugging Face Dataset, e.g. so that we can build a \"crowd-sourced\" dataset:\\n\\n```python\\nimport os\\n\\nHF_TOKEN = os.getenv('HF_TOKEN')\\nhf_writer = gr.HuggingFaceDatasetSaver(HF_TOKEN, \"crowdsourced-calculator-demo\")\\n\\niface = gr.Interface(\\n    calculator,\\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\\n    \"number\",\\n    description=\"Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)\",\\n    allow_flagging=\"manual\",\\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"],\\n    flagging_callback=hf_writer\\n)\\n\\niface.launch()\\n```</td>\n",
       "      <td>What is the purpose of the `flagging_callback` parameter in the Gradio interface?</td>\n",
       "      <td>It allows you to specify what to do with the flagged data, such as piping it into a Hugging Face Dataset.</td>\n",
       "      <td>gradio-app/gradio/blob/main/guides/09_other-tutorials/using-flagging.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  context  \\\n",
       "0  If we go back to the calculator example, the following code will create the interface embedded below it.\\n\\n```python\\niface = gr.Interface(\\n    calculator,\\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\\n    \"number\",\\n    allow_flagging=\"manual\",\\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"]\\n)\\n\\niface.launch()\\n```\\n\\n<gradio-app space=\"gradio/calculator-flagging-options/\"></gradio-app>\\n\\nWhen users click the flag button, the csv file will now include a column indicating the selected option.\\n\\n_flagged/logs.csv_\\n\\n```csv\\nnum1,operation,num2,Output,flag,timestamp\\n5,add,7,-12,wrong sign,2022-02-04 11:40:51.093412\\n6,subtract,1.5,3.5,off by one,2022-02-04 11:42:32.062512\\n```\\n\\n## The HuggingFaceDatasetSaver Callback\\n\\nSometimes, saving the data to a local CSV file doesn't make sense. For example, on Hugging Face\\nSpaces, developers typically don't have access to the underlying ephemeral machine hosting the Gradio\\ndemo. That's why, by default, flagging is turned off in Hugging Face Space. However,\\nyou may want to do something else with the flagged data.\\n\\nWe've made this super easy with the `flagging_callback` parameter.\\n\\nFor example, below we're going to pipe flagged data from our calculator example into a Hugging Face Dataset, e.g. so that we can build a \"crowd-sourced\" dataset:\\n\\n```python\\nimport os\\n\\nHF_TOKEN = os.getenv('HF_TOKEN')\\nhf_writer = gr.HuggingFaceDatasetSaver(HF_TOKEN, \"crowdsourced-calculator-demo\")\\n\\niface = gr.Interface(\\n    calculator,\\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\\n    \"number\",\\n    description=\"Check out the crowd-sourced dataset at: [https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo)\",\\n    allow_flagging=\"manual\",\\n    flagging_options=[\"wrong sign\", \"off by one\", \"other\"],\\n    flagging_callback=hf_writer\\n)\\n\\niface.launch()\\n```   \n",
       "\n",
       "                                                                            question  \\\n",
       "0  What is the purpose of the `flagging_callback` parameter in the Gradio interface?   \n",
       "\n",
       "                                                                                                      answer  \\\n",
       "0  It allows you to specify what to do with the flagged data, such as piping it into a Hugging Face Dataset.   \n",
       "\n",
       "                                                                source_doc  \n",
       "0  gradio-app/gradio/blob/main/guides/09_other-tutorials/using-flagging.md  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(outputs).head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating critique for each QA couple...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:14<00:00,  7.11s/it]\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "for output in tqdm(outputs):\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(\n",
    "            question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]),\n",
    "        ),\n",
    "        \"relevance\": call_llm(\n",
    "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "        \"standalone\": call_llm(\n",
    "            question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "    }\n",
    "    try:\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            score, eval = (\n",
    "                int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
    "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
    "            )\n",
    "            output.update(\n",
    "                {\n",
    "                    f\"{criterion}_score\": score,\n",
    "                    f\"{criterion}_eval\": eval,\n",
    "                }\n",
    "            )\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(\"Generated critiques for each QA couple successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset before filtering:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the purpose of the `flagging_callback` parameter in the Gradio interface?</td>\n",
       "      <td>It allows you to specify what to do with the flagged data, such as piping it into a Hugging Face Dataset.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the name of the game used as an example in the context?</td>\n",
       "      <td>Stardew Valley</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            question  \\\n",
       "0  What is the purpose of the `flagging_callback` parameter in the Gradio interface?   \n",
       "1                    What is the name of the game used as an example in the context?   \n",
       "\n",
       "                                                                                                      answer  \\\n",
       "0  It allows you to specify what to do with the flagged data, such as piping it into a Hugging Face Dataset.   \n",
       "1                                                                                             Stardew Valley   \n",
       "\n",
       "   groundedness_score  relevance_score  standalone_score  \n",
       "0                   5                5                 5  \n",
       "1                   3                2                 1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "Final evaluation dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the purpose of the `flagging_callback` parameter in the Gradio interface?</td>\n",
       "      <td>It allows you to specify what to do with the flagged data, such as piping it into a Hugging Face Dataset.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            question  \\\n",
       "0  What is the purpose of the `flagging_callback` parameter in the Gradio interface?   \n",
       "\n",
       "                                                                                                      answer  \\\n",
       "0  It allows you to specify what to do with the flagged data, such as piping it into a Hugging Face Dataset.   \n",
       "\n",
       "   groundedness_score  relevance_score  standalone_score  \n",
       "0                   5                5                 5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "\n",
    "print(\"Evaluation dataset before filtering:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "generated_questions = generated_questions.loc[\n",
    "    (generated_questions[\"groundedness_score\"] >= 4)\n",
    "    & (generated_questions[\"relevance_score\"] >= 4)\n",
    "    & (generated_questions[\"standalone_score\"] >= 4)\n",
    "]\n",
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(generated_questions, split=\"train\", preserve_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG LLM's From JaneliaGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Load test dataset\"\"\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241m.\u001b[39mload_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm-ric/huggingface_doc_qa_eval\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"Load test dataset\"\"\"\n",
    "eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "class_prefix is deprecated, please use index_name\n",
      "class_prefix is deprecated, please use index_name\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run rag tests + save results\"\"\"\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "from generate_answer import SemanticSearchService\n",
    "from generate_source import SemanticSearchServiceSources\n",
    "weaviate_url = \"http://localhost:8777\"\n",
    "service_src = SemanticSearchServiceSources(weaviate_url)\n",
    "service_ans = SemanticSearchService(weaviate_url)\n",
    "\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer = service_ans.generate_response(question)\n",
    "\n",
    "        relevant_docs = service_src.generate_response(question)\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Eval Prompts\"\"\"\n",
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\n",
    "\n",
    "\n",
    "eval_chat_model = Ollama(model=\"llama3\")\n",
    "evaluator_name = \"llama3\"\n",
    "\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    eval_chat_model,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        feedback, score = [item.strip() for item in eval_result.content.split(\"[RESULT]\")]\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'READER_MODEL_NAME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m embeddings \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthenlper/gte-small\u001b[39m\u001b[38;5;124m\"\u001b[39m]:  \u001b[38;5;66;03m# Add other embeddings as needed\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rerank \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m]:\n\u001b[0;32m----> 7\u001b[0m         settings_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_embeddings:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddings\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_rerank:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrerank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_reader-model:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mREADER_MODEL_NAME\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m         output_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output/rag_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msettings_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning evaluation for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msettings_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'READER_MODEL_NAME' is not defined"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "for chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n",
    "    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n",
    "        for rerank in [True, False]:\n",
    "            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n",
    "            output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "\n",
    "            print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "            print(\"Loading knowledge base embeddings...\")\n",
    "            knowledge_index = load_embeddings(\n",
    "                RAW_KNOWLEDGE_BASE,\n",
    "                chunk_size=chunk_size,\n",
    "                embedding_model_name=embeddings,\n",
    "            )\n",
    "\n",
    "            print(\"Running RAG...\")\n",
    "            reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\") if rerank else None\n",
    "            run_rag_tests(\n",
    "                eval_dataset=eval_dataset,\n",
    "                llm=READER_LLM,\n",
    "                knowledge_index=knowledge_index,\n",
    "                output_file=output_file_name,\n",
    "                reranker=reranker,\n",
    "                verbose=False,\n",
    "                test_settings=settings_name,\n",
    "            )\n",
    "\n",
    "            print(\"Running evaluation...\")\n",
    "            evaluate_answers(\n",
    "                output_file_name,\n",
    "                eval_chat_model,\n",
    "                evaluator_name,\n",
    "                evaluation_prompt_template,\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
